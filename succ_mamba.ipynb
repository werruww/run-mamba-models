{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "606a6805c1b94be99427ff320dc0b402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ecbdbce2c16b40b5bf3cd80ce5a8cf1b",
              "IPY_MODEL_9c23dd5e8b2e4fa9b8549774168a0faa",
              "IPY_MODEL_55060c8f31a444acb67a298d441681ac"
            ],
            "layout": "IPY_MODEL_e7b6130ed2a5474fbff3f748f758d620"
          }
        },
        "ecbdbce2c16b40b5bf3cd80ce5a8cf1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69e0c0744eb8424fa788f48873b31592",
            "placeholder": "​",
            "style": "IPY_MODEL_d6431759c8a344d6a62dd322b543354a",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "9c23dd5e8b2e4fa9b8549774168a0faa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63ef103ae52a46ab8bcb998e3f48ded0",
            "max": 4793,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a759da870cd6452fa712839ed8754918",
            "value": 4793
          }
        },
        "55060c8f31a444acb67a298d441681ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7efc358fcd7341549f84005767b80ccb",
            "placeholder": "​",
            "style": "IPY_MODEL_2d4f55edecb3410f9417764cae4ceb4a",
            "value": " 4.79k/4.79k [00:00&lt;00:00, 373kB/s]"
          }
        },
        "e7b6130ed2a5474fbff3f748f758d620": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69e0c0744eb8424fa788f48873b31592": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6431759c8a344d6a62dd322b543354a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63ef103ae52a46ab8bcb998e3f48ded0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a759da870cd6452fa712839ed8754918": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7efc358fcd7341549f84005767b80ccb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d4f55edecb3410f9417764cae4ceb4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b99e1e4d59dd4ae6a6037c38da1c473d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9b7089f64b8489ebad3f1430ce2e67b",
              "IPY_MODEL_d42d53d432a94724872f497a9b6ced86",
              "IPY_MODEL_6892ee3ac5ab404f9e19f18e85477468"
            ],
            "layout": "IPY_MODEL_8759f4591b814668b3597ff90d48c91f"
          }
        },
        "b9b7089f64b8489ebad3f1430ce2e67b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b711e990bb1646638933caa54a6f95ac",
            "placeholder": "​",
            "style": "IPY_MODEL_7f726f996c8d4526957db9ffcb4a8229",
            "value": "tokenizer.json: 100%"
          }
        },
        "d42d53d432a94724872f497a9b6ced86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e3261aa662149c7b28f1c31e369b110",
            "max": 2113738,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f3cfe14a3844a95ab58965fba15bffa",
            "value": 2113738
          }
        },
        "6892ee3ac5ab404f9e19f18e85477468": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82afad6c4f47431487ad3c645d8b28eb",
            "placeholder": "​",
            "style": "IPY_MODEL_5b9e243ea38f4aec868f9fd3317f2d98",
            "value": " 2.11M/2.11M [00:00&lt;00:00, 7.29MB/s]"
          }
        },
        "8759f4591b814668b3597ff90d48c91f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b711e990bb1646638933caa54a6f95ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f726f996c8d4526957db9ffcb4a8229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e3261aa662149c7b28f1c31e369b110": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f3cfe14a3844a95ab58965fba15bffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82afad6c4f47431487ad3c645d8b28eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b9e243ea38f4aec868f9fd3317f2d98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa06815fc4e843a0b537b97e58ae70ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be6a681d8e6b4eb1af272ceab3247955",
              "IPY_MODEL_3e601b40c01542c581499b0e6a8cc491",
              "IPY_MODEL_1d8a08c7b5e940458bc70286dc07dc6e"
            ],
            "layout": "IPY_MODEL_bee89981d1cc4243b613f88efb692e23"
          }
        },
        "be6a681d8e6b4eb1af272ceab3247955": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c41fe5a9583e4c4f93eda880b65e0aa2",
            "placeholder": "​",
            "style": "IPY_MODEL_95657290fc284e748ac823494ec43c4b",
            "value": "config.json: 100%"
          }
        },
        "3e601b40c01542c581499b0e6a8cc491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecc8565e34504215a6dd33bcc5e7f806",
            "max": 878,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3cea6de6ba9c47ea8ae7eccaa4162509",
            "value": 878
          }
        },
        "1d8a08c7b5e940458bc70286dc07dc6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d37cf84817f343ffa9f29d090916ac4e",
            "placeholder": "​",
            "style": "IPY_MODEL_3e4d1a6259bd4ac7a3540b339ca61853",
            "value": " 878/878 [00:00&lt;00:00, 62.0kB/s]"
          }
        },
        "bee89981d1cc4243b613f88efb692e23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c41fe5a9583e4c4f93eda880b65e0aa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95657290fc284e748ac823494ec43c4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecc8565e34504215a6dd33bcc5e7f806": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cea6de6ba9c47ea8ae7eccaa4162509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d37cf84817f343ffa9f29d090916ac4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e4d1a6259bd4ac7a3540b339ca61853": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6faa830ce8d464b9d12844353172ddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e3a8ad10e944f36832c8190a398ae1a",
              "IPY_MODEL_61615c5161a4458d856d74aa64af1fc0",
              "IPY_MODEL_71df0f15a7854a48a74f20d0ccffc7d0"
            ],
            "layout": "IPY_MODEL_025cf267fc124ce8821665f0e8f5e263"
          }
        },
        "3e3a8ad10e944f36832c8190a398ae1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_752a268e8d984781b374400cdf0b1d24",
            "placeholder": "​",
            "style": "IPY_MODEL_c755a20625274254b5d42106068cca1a",
            "value": "model.safetensors: 100%"
          }
        },
        "61615c5161a4458d856d74aa64af1fc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93054d2774264fbbbb4d1603a344f76c",
            "max": 3172869936,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7192e94920be4d609edb753e4c71b066",
            "value": 3172869936
          }
        },
        "71df0f15a7854a48a74f20d0ccffc7d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcde3d63d5c7482e8dbe035055bd2be3",
            "placeholder": "​",
            "style": "IPY_MODEL_7215cc2a6c104210a4a1db6bcc653dda",
            "value": " 3.17G/3.17G [01:16&lt;00:00, 38.4MB/s]"
          }
        },
        "025cf267fc124ce8821665f0e8f5e263": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "752a268e8d984781b374400cdf0b1d24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c755a20625274254b5d42106068cca1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93054d2774264fbbbb4d1603a344f76c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7192e94920be4d609edb753e4c71b066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bcde3d63d5c7482e8dbe035055bd2be3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7215cc2a6c104210a4a1db6bcc653dda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6b2ab80015c4d6784313af508b301fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf63a49dcb6347c78e8d32cdf6ae4430",
              "IPY_MODEL_3a6b182ad6b543cebc6893c110475e58",
              "IPY_MODEL_535416cdb4064191800689f432c27880"
            ],
            "layout": "IPY_MODEL_452f85afc1a94982bc3a008fbe0f2e65"
          }
        },
        "cf63a49dcb6347c78e8d32cdf6ae4430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bd102b823c444e6ac19e1332788fabd",
            "placeholder": "​",
            "style": "IPY_MODEL_2695efaec3d54f5c9b8018656903e054",
            "value": "generation_config.json: 100%"
          }
        },
        "3a6b182ad6b543cebc6893c110475e58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_771ed946dbbd4fecaaab42b3bd3971a6",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc96f1c29ff44da1a901b8a8a09a06fb",
            "value": 137
          }
        },
        "535416cdb4064191800689f432c27880": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8745c7927fe41b89f1d0134ed57b9be",
            "placeholder": "​",
            "style": "IPY_MODEL_3ea2eb88556a470bbeac56ead0b49f00",
            "value": " 137/137 [00:00&lt;00:00, 6.67kB/s]"
          }
        },
        "452f85afc1a94982bc3a008fbe0f2e65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bd102b823c444e6ac19e1332788fabd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2695efaec3d54f5c9b8018656903e054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "771ed946dbbd4fecaaab42b3bd3971a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc96f1c29ff44da1a901b8a8a09a06fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8745c7927fe41b89f1d0134ed57b9be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ea2eb88556a470bbeac56ead0b49f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HufkuJqFM6ls"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B4sFWrIVM78T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/state-spaces/mamba/releases/download/v2.2.4/mamba_ssm-2.2.4+cu12torch2.6cxx11abiTRUE-cp311-cp311-linux_x86_64.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUFeBj6yM8DQ",
        "outputId": "6f8254c9-9773-4b48-889f-8e5e6947c398"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-19 21:53:48--  https://github.com/state-spaces/mamba/releases/download/v2.2.4/mamba_ssm-2.2.4+cu12torch2.6cxx11abiTRUE-cp311-cp311-linux_x86_64.whl\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/725839295/9b1d5f6d-d321-4f71-a363-a6ad96e81d2f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250219%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250219T215348Z&X-Amz-Expires=300&X-Amz-Signature=ff6000591a25cc3a810003efeb75f7222e81fbc83bd1efca1e8660b1cd610eba&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmamba_ssm-2.2.4%2Bcu12torch2.6cxx11abiTRUE-cp311-cp311-linux_x86_64.whl&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-02-19 21:53:48--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/725839295/9b1d5f6d-d321-4f71-a363-a6ad96e81d2f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250219%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250219T215348Z&X-Amz-Expires=300&X-Amz-Signature=ff6000591a25cc3a810003efeb75f7222e81fbc83bd1efca1e8660b1cd610eba&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmamba_ssm-2.2.4%2Bcu12torch2.6cxx11abiTRUE-cp311-cp311-linux_x86_64.whl&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 323673357 (309M) [application/octet-stream]\n",
            "Saving to: ‘mamba_ssm-2.2.4+cu12torch2.6cxx11abiTRUE-cp311-cp311-linux_x86_64.whl’\n",
            "\n",
            "mamba_ssm-2.2.4+cu1 100%[===================>] 308.68M  35.6MB/s    in 8.1s    \n",
            "\n",
            "2025-02-19 21:53:56 (38.2 MB/s) - ‘mamba_ssm-2.2.4+cu12torch2.6cxx11abiTRUE-cp311-cp311-linux_x86_64.whl’ saved [323673357/323673357]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install /content/mamba_ssm-2.2.4+cu12torch2.6cxx11abiTRUE-cp311-cp311-linux_x86_64.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Efegb2P8M9_q",
        "outputId": "48a36d55-62a6-4991-956f-db983ccc51b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./mamba_ssm-2.2.4+cu12torch2.6cxx11abiTRUE-cp311-cp311-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (2.5.1+cu124)\n",
            "Collecting ninja (from mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE)\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (0.8.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (4.48.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (24.2)\n",
            "Requirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.11/dist-packages (from mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm==2.2.4+cu12torch2.6cxx11abiTRUE) (2025.1.31)\n",
            "Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, mamba-ssm\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed mamba-ssm-2.2.4 ninja-1.11.1.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.5.0.post8/causal_conv1d-1.5.0.post8+cu11torch2.1cxx11abiFALSE-cp311-cp311-linux_x86_64.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjYiaUrBNCaA",
        "outputId": "73be6c2a-6ebd-4395-aee0-534988ca1b5f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-19 21:56:04--  https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.5.0.post8/causal_conv1d-1.5.0.post8+cu11torch2.1cxx11abiFALSE-cp311-cp311-linux_x86_64.whl\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/726718359/1c531793-9d37-4547-a385-96f51b0b51ac?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250219%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250219T215604Z&X-Amz-Expires=300&X-Amz-Signature=762a1427a34e82a936484e1ad6375787e50ebac70bbd791f8213060f962fafd5&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcausal_conv1d-1.5.0.post8%2Bcu11torch2.1cxx11abiFALSE-cp311-cp311-linux_x86_64.whl&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-02-19 21:56:05--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/726718359/1c531793-9d37-4547-a385-96f51b0b51ac?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250219%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250219T215604Z&X-Amz-Expires=300&X-Amz-Signature=762a1427a34e82a936484e1ad6375787e50ebac70bbd791f8213060f962fafd5&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcausal_conv1d-1.5.0.post8%2Bcu11torch2.1cxx11abiFALSE-cp311-cp311-linux_x86_64.whl&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 102826687 (98M) [application/octet-stream]\n",
            "Saving to: ‘causal_conv1d-1.5.0.post8+cu11torch2.1cxx11abiFALSE-cp311-cp311-linux_x86_64.whl’\n",
            "\n",
            "causal_conv1d-1.5.0 100%[===================>]  98.06M  41.6MB/s    in 2.4s    \n",
            "\n",
            "2025-02-19 21:56:07 (41.6 MB/s) - ‘causal_conv1d-1.5.0.post8+cu11torch2.1cxx11abiFALSE-cp311-cp311-linux_x86_64.whl’ saved [102826687/102826687]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install /content/causal_conv1d-1.5.0.post8+cu11torch2.1cxx11abiFALSE-cp311-cp311-linux_x86_64.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcwVo1VaNYzg",
        "outputId": "77591b9d-c996-41af-daa1-3615bed608da"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./causal_conv1d-1.5.0.post8+cu11torch2.1cxx11abiFALSE-cp311-cp311-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (2.5.1+cu124)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (24.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (1.11.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->causal-conv1d==1.5.0.post8+cu11torch2.1cxx11abiFALSE) (3.0.2)\n",
            "Installing collected packages: causal-conv1d\n",
            "Successfully installed causal-conv1d-1.5.0.post8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from mamba_ssm import Mamba\n",
        "\n",
        "batch, length, dim = 2, 64, 16\n",
        "x = torch.randn(batch, length, dim).to(\"cuda\")\n",
        "model = Mamba(\n",
        "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
        "    d_model=dim, # Model dimension d_model\n",
        "    d_state=16,  # SSM state expansion factor\n",
        "    d_conv=4,    # Local convolution width\n",
        "    expand=2,    # Block expansion factor\n",
        ").to(\"cuda\")\n",
        "y = model(x)\n",
        "assert y.shape == x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "HrdqfsMlNkC3",
        "outputId": "e4bf4849-250b-4798-b21a-d99e9457f5d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "/usr/local/lib/python3.11/dist-packages/selective_scan_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c107WarningC1ESt7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEERKNS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEb",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e160b7215ca2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMamba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mamba_ssm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.2.4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselective_scan_interface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mselective_scan_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmamba_inner_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmamba_simple\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMamba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmamba2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMamba2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mamba_ssm/ops/selective_scan_interface.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_layer_norm_fwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mselective_scan_cuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.11/dist-packages/selective_scan_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c107WarningC1ESt7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEERKNS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEb",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mamba_ssm import Mamba2\n",
        "model = Mamba2(\n",
        "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
        "    d_model=dim, # Model dimension d_model\n",
        "    d_state=64,  # SSM state expansion factor, typically 64 or 128\n",
        "    d_conv=4,    # Local convolution width\n",
        "    expand=2,    # Block expansion factor\n",
        ").to(\"cuda\")\n",
        "y = model(x)\n",
        "assert y.shape == x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "g5B22uU7NqIv",
        "outputId": "6250beb4-ac4e-48f7-ceb7-b530c112b88b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "/usr/local/lib/python3.11/dist-packages/selective_scan_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c107WarningC1ESt7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEERKNS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEb",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-dbc7dc29589d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMamba2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m model = Mamba2(\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# This module uses roughly 3 * expand * d_model^2 parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Model dimension d_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0md_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# SSM state expansion factor, typically 64 or 128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mamba_ssm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.2.4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselective_scan_interface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mselective_scan_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmamba_inner_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmamba_simple\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMamba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmamba2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMamba2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mamba_ssm/ops/selective_scan_interface.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_layer_norm_fwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mselective_scan_cuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.11/dist-packages/selective_scan_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c107WarningC1ESt7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEERKNS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEb",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mamba-ssm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoA5MK-aN8so",
        "outputId": "b0f7736f-5f4a-4afe-a5b8-35e5feb42365"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mamba-ssm in /usr/local/lib/python3.11/dist-packages (2.2.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (2.5.1+cu124)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (1.11.1.3)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (0.8.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (4.48.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (24.2)\n",
            "Requirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->mamba-ssm) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->mamba-ssm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install causal-conv1d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g99c6oHhN9Wa",
        "outputId": "33814017-501c-4977-d91b-f6559b710049"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: causal-conv1d in /usr/local/lib/python3.11/dist-packages (1.5.0.post8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from causal-conv1d) (2.5.1+cu124)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from causal-conv1d) (24.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from causal-conv1d) (1.11.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->causal-conv1d) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->causal-conv1d) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->causal-conv1d) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-p-TG_lOAgX",
        "outputId": "ef6c1f11-eabb-4982-f055-9304cdcd1d73"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-krx2hdvr\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-krx2hdvr\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 31bb662db19355e56b3125ab5c48b780ec13111c\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.0.dev0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.0.dev0) (2025.1.31)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.50.0.dev0-py3-none-any.whl size=10772988 sha256=0ce1c104ab70f1fec2f334ba490f141fa827ba148f1b99a03558c05ef2e78977\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-m48x86fu/wheels/32/4b/78/f195c684dd3a9ed21f3b39fe8f85b48df7918581b6437be143\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.3\n",
            "    Uninstalling transformers-4.48.3:\n",
            "      Successfully uninstalled transformers-4.48.3\n",
            "Successfully installed transformers-4.50.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mamba_ssm import Mamba2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "68BPr6zHOK4_",
        "outputId": "c5ce3f01-3155-4cb2-ddbb-080d0184be24"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "/usr/local/lib/python3.11/dist-packages/selective_scan_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c107WarningC1ESt7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEERKNS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEb",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ec3b73df2296>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMamba2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mamba_ssm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.2.4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselective_scan_interface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mselective_scan_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmamba_inner_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmamba_simple\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMamba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmamba2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMamba2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mamba_ssm/ops/selective_scan_interface.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_layer_norm_fwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mselective_scan_cuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.11/dist-packages/selective_scan_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c107WarningC1ESt7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEERKNS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEb",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from mamba_ssm import Mamba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "5GsnvnEfOZOw",
        "outputId": "cc199b6b-c1e4-4a4d-9c61-a2c7e742d86e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "/usr/local/lib/python3.11/dist-packages/selective_scan_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c107WarningC1ESt7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEERKNS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEb",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9f5a5b3b7567>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMamba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mamba_ssm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.2.4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselective_scan_interface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mselective_scan_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmamba_inner_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmamba_simple\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMamba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmamba2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMamba2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mamba_ssm/ops/selective_scan_interface.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_layer_norm_fwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mselective_scan_cuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.11/dist-packages/selective_scan_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c107WarningC1ESt7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEERKNS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEb",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import Mamba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "NGmKxqzsOmkR",
        "outputId": "8309b374-1345-4b52-9837-f2caf74a3019"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'Mamba'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f64c3f0cce14>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mMamba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Mamba'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rMpHsgK-Om5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "git clone --branch BambaArchitecture git@github.com:gabe-l-hart/llama.cpp.git\n",
        "cd llama.cpp\n",
        "mkdir build\n",
        "cd build\n",
        "# NOTE: To build with debug symbols and extra logging, use CMAKE_BUILD_TYPE=Debug\n",
        "cmake .. -DCMAKE_BUILD_TYPE=Release\n",
        "make -j"
      ],
      "metadata": {
        "id": "AzCV4cTIPDpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/gabe-l-hart/llama.cpp/releases/download/b4358/llama-b4358-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1ROGYcoPKcQ",
        "outputId": "20241c05-3f9a-4431-c260-88428c27efc2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-19 22:03:26--  https://github.com/gabe-l-hart/llama.cpp/releases/download/b4358/llama-b4358-bin-ubuntu-x64.zip\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/768340132/c23a4373-c2f8-4247-b7fd-a9934820257d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250219%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250219T220327Z&X-Amz-Expires=300&X-Amz-Signature=20cc5fa6b6b4c011a5c539d1fa22342ea16c877316af15982bfb57e27de1537a&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllama-b4358-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-02-19 22:03:27--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/768340132/c23a4373-c2f8-4247-b7fd-a9934820257d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250219%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250219T220327Z&X-Amz-Expires=300&X-Amz-Signature=20cc5fa6b6b4c011a5c539d1fa22342ea16c877316af15982bfb57e27de1537a&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllama-b4358-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68808977 (66M) [application/octet-stream]\n",
            "Saving to: ‘llama-b4358-bin-ubuntu-x64.zip’\n",
            "\n",
            "llama-b4358-bin-ubu 100%[===================>]  65.62M  33.8MB/s    in 1.9s    \n",
            "\n",
            "2025-02-19 22:03:29 (33.8 MB/s) - ‘llama-b4358-bin-ubuntu-x64.zip’ saved [68808977/68808977]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip llama-b4358-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x6Ke6MLPL4w",
        "outputId": "f97489a5-f696-462e-d61d-e36e9be2b348"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  llama-b4358-bin-ubuntu-x64.zip\n",
            "  inflating: build/bin/LICENSE       \n",
            "  inflating: build/bin/llama-batched  \n",
            "  inflating: build/bin/llama-batched-bench  \n",
            "  inflating: build/bin/llama-bench   \n",
            "  inflating: build/bin/llama-cli     \n",
            "  inflating: build/bin/llama-convert-llama2c-to-ggml  \n",
            "  inflating: build/bin/llama-cvector-generator  \n",
            "  inflating: build/bin/llama-embedding  \n",
            "  inflating: build/bin/llama-eval-callback  \n",
            "  inflating: build/bin/llama-export-lora  \n",
            "  inflating: build/bin/llama-gbnf-validator  \n",
            "  inflating: build/bin/llama-gen-docs  \n",
            "  inflating: build/bin/llama-gguf    \n",
            "  inflating: build/bin/llama-gguf-hash  \n",
            "  inflating: build/bin/llama-gguf-split  \n",
            "  inflating: build/bin/llama-gritlm  \n",
            "  inflating: build/bin/llama-imatrix  \n",
            "  inflating: build/bin/llama-infill  \n",
            "  inflating: build/bin/llama-llava-cli  \n",
            "  inflating: build/bin/llama-lookahead  \n",
            "  inflating: build/bin/llama-lookup  \n",
            "  inflating: build/bin/llama-lookup-create  \n",
            "  inflating: build/bin/llama-lookup-merge  \n",
            "  inflating: build/bin/llama-lookup-stats  \n",
            "  inflating: build/bin/llama-minicpmv-cli  \n",
            "  inflating: build/bin/llama-parallel  \n",
            "  inflating: build/bin/llama-passkey  \n",
            "  inflating: build/bin/llama-perplexity  \n",
            "  inflating: build/bin/llama-q8dot   \n",
            "  inflating: build/bin/llama-quantize  \n",
            "  inflating: build/bin/llama-quantize-stats  \n",
            "  inflating: build/bin/llama-qwen2vl-cli  \n",
            "  inflating: build/bin/llama-retrieval  \n",
            "  inflating: build/bin/llama-run     \n",
            "  inflating: build/bin/llama-save-load-state  \n",
            "  inflating: build/bin/llama-server  \n",
            "  inflating: build/bin/llama-simple  \n",
            "  inflating: build/bin/llama-simple-chat  \n",
            "  inflating: build/bin/llama-speculative  \n",
            "  inflating: build/bin/llama-speculative-simple  \n",
            "  inflating: build/bin/llama-tokenize  \n",
            "  inflating: build/bin/llama-tts     \n",
            "  inflating: build/bin/llama-vdot    \n",
            "  inflating: build/bin/rpc-server    \n",
            "  inflating: build/bin/test-arg-parser  \n",
            "  inflating: build/bin/test-autorelease  \n",
            "  inflating: build/bin/test-backend-ops  \n",
            "  inflating: build/bin/test-barrier  \n",
            "  inflating: build/bin/test-c        \n",
            "  inflating: build/bin/test-chat-template  \n",
            "  inflating: build/bin/test-gguf     \n",
            "  inflating: build/bin/test-grammar-integration  \n",
            "  inflating: build/bin/test-grammar-parser  \n",
            "  inflating: build/bin/test-json-schema-to-grammar  \n",
            "  inflating: build/bin/test-llama-grammar  \n",
            "  inflating: build/bin/test-log      \n",
            "  inflating: build/bin/test-model-load-cancel  \n",
            "  inflating: build/bin/test-quantize-fns  \n",
            "  inflating: build/bin/test-quantize-perf  \n",
            "  inflating: build/bin/test-rope     \n",
            "  inflating: build/bin/test-sampling  \n",
            "  inflating: build/bin/test-tokenizer-0  \n",
            "  inflating: build/bin/test-tokenizer-1-bpe  \n",
            "  inflating: build/bin/test-tokenizer-1-spm  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdNFl5TBPUfo",
        "outputId": "a26bef74-7e10-4d30-e060-16855e66e3ce"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- common params -----\n",
            "\n",
            "-h,    --help, --usage                  print usage and exit\n",
            "--version                               show version and build info\n",
            "--verbose-prompt                        print a verbose prompt before generation (default: false)\n",
            "-t,    --threads N                      number of threads to use during generation (default: -1)\n",
            "                                        (env: LLAMA_ARG_THREADS)\n",
            "-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:\n",
            "                                        same as --threads)\n",
            "-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range\n",
            "                                        (default: \"\")\n",
            "-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask\n",
            "--cpu-strict <0|1>                      use strict CPU placement (default: 0)\n",
            "--prio N                                set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)\n",
            "-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch\n",
            "                                        (default: same as --cpu-mask)\n",
            "-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch\n",
            "--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)\n",
            "--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)\n",
            "-c,    --ctx-size N                     size of the prompt context (default: 4096, 0 = loaded from model)\n",
            "                                        (env: LLAMA_ARG_CTX_SIZE)\n",
            "-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity, -2 = until\n",
            "                                        context filled)\n",
            "                                        (env: LLAMA_ARG_N_PREDICT)\n",
            "-b,    --batch-size N                   logical maximum batch size (default: 2048)\n",
            "                                        (env: LLAMA_ARG_BATCH)\n",
            "-ub,   --ubatch-size N                  physical maximum batch size (default: 512)\n",
            "                                        (env: LLAMA_ARG_UBATCH)\n",
            "--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =\n",
            "                                        all)\n",
            "-fa,   --flash-attn                     enable Flash Attention (default: disabled)\n",
            "                                        (env: LLAMA_ARG_FLASH_ATTN)\n",
            "-p,    --prompt PROMPT                  prompt to start generation with\n",
            "                                        if -cnv is set, this will be used as system prompt\n",
            "--no-perf                               disable internal libllama performance timings (default: false)\n",
            "                                        (env: LLAMA_ARG_NO_PERF)\n",
            "-f,    --file FNAME                     a file containing the prompt (default: none)\n",
            "-bf,   --binary-file FNAME              binary file containing the prompt (default: none)\n",
            "-e,    --escape                         process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\) (default: true)\n",
            "--no-escape                             do not process escape sequences\n",
            "--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by\n",
            "                                        the model\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)\n",
            "--rope-scale N                          RoPE context scaling factor, expands context by a factor of N\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALE)\n",
            "--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from\n",
            "                                        model)\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)\n",
            "--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)\n",
            "--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training\n",
            "                                        context size)\n",
            "                                        (env: LLAMA_ARG_YARN_ORIG_CTX)\n",
            "--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full\n",
            "                                        interpolation)\n",
            "                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)\n",
            "--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)\n",
            "--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_SLOW)\n",
            "--yarn-beta-fast N                      YaRN: low correction dim or beta (default: 32.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_FAST)\n",
            "-dkvc, --dump-kv-cache                  verbose print of the KV cache\n",
            "-nkvo, --no-kv-offload                  disable KV offload\n",
            "                                        (env: LLAMA_ARG_NO_KV_OFFLOAD)\n",
            "-ctk,  --cache-type-k TYPE              KV cache data type for K\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K)\n",
            "-ctv,  --cache-type-v TYPE              KV cache data type for V\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V)\n",
            "-dt,   --defrag-thold N                 KV cache defragmentation threshold (default: 0.1, < 0 - disabled)\n",
            "                                        (env: LLAMA_ARG_DEFRAG_THOLD)\n",
            "-np,   --parallel N                     number of parallel sequences to decode (default: 1)\n",
            "                                        (env: LLAMA_ARG_N_PARALLEL)\n",
            "--rpc SERVERS                           comma separated list of RPC servers\n",
            "                                        (env: LLAMA_ARG_RPC)\n",
            "--mlock                                 force system to keep model in RAM rather than swapping or compressing\n",
            "                                        (env: LLAMA_ARG_MLOCK)\n",
            "--no-mmap                               do not memory-map model (slower load but may reduce pageouts if not\n",
            "                                        using mlock)\n",
            "                                        (env: LLAMA_ARG_NO_MMAP)\n",
            "--numa TYPE                             attempt optimizations that help on some NUMA systems\n",
            "                                        - distribute: spread execution evenly over all nodes\n",
            "                                        - isolate: only spawn threads on CPUs on the node that execution\n",
            "                                        started on\n",
            "                                        - numactl: use the CPU map provided by numactl\n",
            "                                        if run without this previously, it is recommended to drop the system\n",
            "                                        page cache before using this\n",
            "                                        see https://github.com/ggerganov/llama.cpp/issues/1437\n",
            "                                        (env: LLAMA_ARG_NUMA)\n",
            "-dev,  --device <dev1,dev2,..>          comma-separated list of devices to use for offloading (none = don't\n",
            "                                        offload)\n",
            "                                        use --list-devices to see a list of available devices\n",
            "                                        (env: LLAMA_ARG_DEVICE)\n",
            "--list-devices                          print list of available devices and exit\n",
            "-ngl,  --gpu-layers, --n-gpu-layers N   number of layers to store in VRAM\n",
            "                                        (env: LLAMA_ARG_N_GPU_LAYERS)\n",
            "-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\n",
            "                                        - none: use one GPU only\n",
            "                                        - layer (default): split layers and KV across GPUs\n",
            "                                        - row: split rows across GPUs\n",
            "                                        (env: LLAMA_ARG_SPLIT_MODE)\n",
            "-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of\n",
            "                                        proportions, e.g. 3,1\n",
            "                                        (env: LLAMA_ARG_TENSOR_SPLIT)\n",
            "-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for\n",
            "                                        intermediate results and KV (with split-mode = row) (default: 0)\n",
            "                                        (env: LLAMA_ARG_MAIN_GPU)\n",
            "--check-tensors                         check model tensor data for invalid values (default: false)\n",
            "--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified\n",
            "                                        multiple times.\n",
            "                                        types: int, float, bool, str. example: --override-kv\n",
            "                                        tokenizer.ggml.add_bos_token=bool:false\n",
            "--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)\n",
            "--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use\n",
            "                                        multiple adapters)\n",
            "--control-vector FNAME                  add a control vector\n",
            "                                        note: this argument can be repeated to add multiple control vectors\n",
            "--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE\n",
            "                                        note: this argument can be repeated to add multiple scaled control\n",
            "                                        vectors\n",
            "--control-vector-layer-range START END\n",
            "                                        layer range to apply the control vector(s) to, start and end inclusive\n",
            "-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`\n",
            "                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)\n",
            "                                        (env: LLAMA_ARG_MODEL)\n",
            "-mu,   --model-url MODEL_URL            model download url (default: unused)\n",
            "                                        (env: LLAMA_ARG_MODEL_URL)\n",
            "-hfr,  --hf-repo REPO                   Hugging Face model repository (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO)\n",
            "-hff,  --hf-file FILE                   Hugging Face model file (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE)\n",
            "-hfrv, --hf-repo-v REPO                 Hugging Face model repository for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO_V)\n",
            "-hffv, --hf-file-v FILE                 Hugging Face model file for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE_V)\n",
            "-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment\n",
            "                                        variable)\n",
            "                                        (env: HF_TOKEN)\n",
            "--log-disable                           Log disable\n",
            "--log-file FNAME                        Log to file\n",
            "--log-colors                            Enable colored logging\n",
            "                                        (env: LLAMA_LOG_COLORS)\n",
            "-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for\n",
            "                                        debugging)\n",
            "-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be\n",
            "                                        ignored.\n",
            "                                        (env: LLAMA_LOG_VERBOSITY)\n",
            "--log-prefix                            Enable prefx in log messages\n",
            "                                        (env: LLAMA_LOG_PREFIX)\n",
            "--log-timestamps                        Enable timestamps in log messages\n",
            "                                        (env: LLAMA_LOG_TIMESTAMPS)\n",
            "\n",
            "\n",
            "----- sampling params -----\n",
            "\n",
            "--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by\n",
            "                                        ';'\n",
            "                                        (default: penalties;dry;top_k;typ_p;top_p;min_p;xtc;temperature)\n",
            "-s,    --seed SEED                      RNG seed (default: -1, use random seed for -1)\n",
            "--sampling-seq, --sampler-seq SEQUENCE\n",
            "                                        simplified sequence for samplers that will be used (default: edkypmxt)\n",
            "--ignore-eos                            ignore end of stream token and continue generating (implies\n",
            "                                        --logit-bias EOS-inf)\n",
            "--temp N                                temperature (default: 0.8)\n",
            "--top-k N                               top-k sampling (default: 40, 0 = disabled)\n",
            "--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)\n",
            "--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)\n",
            "--xtc-probability N                     xtc probability (default: 0.0, 0.0 = disabled)\n",
            "--xtc-threshold N                       xtc threshold (default: 0.1, 1.0 = disabled)\n",
            "--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
            "--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1\n",
            "                                        = ctx_size)\n",
            "--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
            "--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
            "--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
            "--dry-multiplier N                      set DRY sampling multiplier (default: 0.0, 0.0 = disabled)\n",
            "--dry-base N                            set DRY sampling base value (default: 1.75)\n",
            "--dry-allowed-length N                  set allowed length for DRY sampling (default: 2)\n",
            "--dry-penalty-last-n N                  set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 =\n",
            "                                        context size)\n",
            "--dry-sequence-breaker STRING           add sequence breaker for DRY sampling, clearing out default breakers\n",
            "                                        ('\\n', ':', '\"', '*') in the process; use \"none\" to not use any\n",
            "                                        sequence breakers\n",
            "--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
            "--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)\n",
            "--mirostat N                            use Mirostat sampling.\n",
            "                                        Top K, Nucleus and Locally Typical samplers are ignored if used.\n",
            "                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
            "--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)\n",
            "--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)\n",
            "-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,\n",
            "                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
            "                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
            "--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/\n",
            "                                        dir) (default: '')\n",
            "--grammar-file FNAME                    file to read grammar from\n",
            "-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.\n",
            "                                        `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "\n",
            "\n",
            "----- example-specific params -----\n",
            "\n",
            "--no-display-prompt                     don't print prompt at generation (default: false)\n",
            "-co,   --color                          colorise output to distinguish prompt and user input from generations\n",
            "                                        (default: false)\n",
            "--no-context-shift                      disables context shift on inifinite text generation (default:\n",
            "                                        disabled)\n",
            "                                        (env: LLAMA_ARG_NO_CONTEXT_SHIFT)\n",
            "-ptc,  --print-token-count N            print token count every N tokens (default: -1)\n",
            "--prompt-cache FNAME                    file to cache prompt state for faster startup (default: none)\n",
            "--prompt-cache-all                      if specified, saves user input and generations to cache as well\n",
            "--prompt-cache-ro                       if specified, uses the prompt cache but does not update it\n",
            "-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode\n",
            "-sp,   --special                        special tokens output enabled (default: false)\n",
            "-cnv,  --conversation                   run in conversation mode:\n",
            "                                        - does not print special tokens and suffix/prefix\n",
            "                                        - interactive mode is also enabled\n",
            "                                        (default: false)\n",
            "-i,    --interactive                    run in interactive mode (default: false)\n",
            "-if,   --interactive-first              run in interactive mode and wait for input right away (default: false)\n",
            "-mli,  --multiline-input                allows you to write or paste multiple lines without ending each in '\\'\n",
            "--in-prefix-bos                         prefix BOS to user inputs, preceding the `--in-prefix` string\n",
            "--in-prefix STRING                      string to prefix user inputs with (default: empty)\n",
            "--in-suffix STRING                      string to suffix after user inputs with (default: empty)\n",
            "--no-warmup                             skip warming up the model with an empty run\n",
            "-gan,  --grp-attn-n N                   group-attention factor (default: 1)\n",
            "                                        (env: LLAMA_ARG_GRP_ATTN_N)\n",
            "-gaw,  --grp-attn-w N                   group-attention width (default: 512)\n",
            "                                        (env: LLAMA_ARG_GRP_ATTN_W)\n",
            "--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's\n",
            "                                        metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        list of built-in templates:\n",
            "                                        chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2, exaone3,\n",
            "                                        gemma, gigachat, granite, llama2, llama2-sys, llama2-sys-bos,\n",
            "                                        llama2-sys-strip, llama3, minicpm, mistral-v1, mistral-v3,\n",
            "                                        mistral-v3-tekken, mistral-v7, monarch, openchat, orion, phi3,\n",
            "                                        rwkv-world, vicuna, vicuna-orca, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE)\n",
            "--simple-io                             use basic IO for better compatibility in subprocesses and limited\n",
            "                                        consoles\n",
            "\n",
            "example usage:\n",
            "\n",
            "  text generation:     build/bin/llama-cli -m your_model.gguf -p \"I believe the meaning of life is\" -n 128\n",
            "\n",
            "  chat (conversation): build/bin/llama-cli -m your_model.gguf -p \"You are a helpful assistant\" -cnv\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers@main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4bVbFEfPVi3",
        "outputId": "9652d057-4fc4-4c9c-8adb-98fa45984564"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers@main\n",
            "  Cloning https://github.com/huggingface/transformers (to revision main) to /tmp/pip-req-build-73vi7jmb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-73vi7jmb\n",
            "  Resolved https://github.com/huggingface/transformers to commit 31bb662db19355e56b3125ab5c48b780ec13111c\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.0.dev0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.0.dev0) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال"
      ],
      "metadata": {
        "id": "HgtYPdr5QRJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-790m-hf\")\n",
        "model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-790m-hf\")\n",
        "input_ids = tokenizer(\"Hey how are you doing?\", return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "out = model.generate(input_ids, max_new_tokens=10)\n",
        "print(tokenizer.batch_decode(out))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371,
          "referenced_widgets": [
            "606a6805c1b94be99427ff320dc0b402",
            "ecbdbce2c16b40b5bf3cd80ce5a8cf1b",
            "9c23dd5e8b2e4fa9b8549774168a0faa",
            "55060c8f31a444acb67a298d441681ac",
            "e7b6130ed2a5474fbff3f748f758d620",
            "69e0c0744eb8424fa788f48873b31592",
            "d6431759c8a344d6a62dd322b543354a",
            "63ef103ae52a46ab8bcb998e3f48ded0",
            "a759da870cd6452fa712839ed8754918",
            "7efc358fcd7341549f84005767b80ccb",
            "2d4f55edecb3410f9417764cae4ceb4a",
            "b99e1e4d59dd4ae6a6037c38da1c473d",
            "b9b7089f64b8489ebad3f1430ce2e67b",
            "d42d53d432a94724872f497a9b6ced86",
            "6892ee3ac5ab404f9e19f18e85477468",
            "8759f4591b814668b3597ff90d48c91f",
            "b711e990bb1646638933caa54a6f95ac",
            "7f726f996c8d4526957db9ffcb4a8229",
            "8e3261aa662149c7b28f1c31e369b110",
            "2f3cfe14a3844a95ab58965fba15bffa",
            "82afad6c4f47431487ad3c645d8b28eb",
            "5b9e243ea38f4aec868f9fd3317f2d98",
            "fa06815fc4e843a0b537b97e58ae70ab",
            "be6a681d8e6b4eb1af272ceab3247955",
            "3e601b40c01542c581499b0e6a8cc491",
            "1d8a08c7b5e940458bc70286dc07dc6e",
            "bee89981d1cc4243b613f88efb692e23",
            "c41fe5a9583e4c4f93eda880b65e0aa2",
            "95657290fc284e748ac823494ec43c4b",
            "ecc8565e34504215a6dd33bcc5e7f806",
            "3cea6de6ba9c47ea8ae7eccaa4162509",
            "d37cf84817f343ffa9f29d090916ac4e",
            "3e4d1a6259bd4ac7a3540b339ca61853",
            "d6faa830ce8d464b9d12844353172ddc",
            "3e3a8ad10e944f36832c8190a398ae1a",
            "61615c5161a4458d856d74aa64af1fc0",
            "71df0f15a7854a48a74f20d0ccffc7d0",
            "025cf267fc124ce8821665f0e8f5e263",
            "752a268e8d984781b374400cdf0b1d24",
            "c755a20625274254b5d42106068cca1a",
            "93054d2774264fbbbb4d1603a344f76c",
            "7192e94920be4d609edb753e4c71b066",
            "bcde3d63d5c7482e8dbe035055bd2be3",
            "7215cc2a6c104210a4a1db6bcc653dda",
            "d6b2ab80015c4d6784313af508b301fd",
            "cf63a49dcb6347c78e8d32cdf6ae4430",
            "3a6b182ad6b543cebc6893c110475e58",
            "535416cdb4064191800689f432c27880",
            "452f85afc1a94982bc3a008fbe0f2e65",
            "5bd102b823c444e6ac19e1332788fabd",
            "2695efaec3d54f5c9b8018656903e054",
            "771ed946dbbd4fecaaab42b3bd3971a6",
            "cc96f1c29ff44da1a901b8a8a09a06fb",
            "c8745c7927fe41b89f1d0134ed57b9be",
            "3ea2eb88556a470bbeac56ead0b49f00"
          ]
        },
        "id": "weNVwZX2PvJw",
        "outputId": "e7647174-9ab5-4e80-e3ed-51b8e1358c55"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/4.79k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "606a6805c1b94be99427ff320dc0b402"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b99e1e4d59dd4ae6a6037c38da1c473d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa06815fc4e843a0b537b97e58ae70ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.17G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6faa830ce8d464b9d12844353172ddc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6b2ab80015c4d6784313af508b301fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "The 'batch_size' argument of MambaCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Hey how are you doing?\\n\\nI'm good.\\n\\nHow are\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -h"
      ],
      "metadata": {
        "id": "zkotDZj_QBHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./convert_hf_to_gguf.py /path/to/bamba-model --outfile /path/to/bamba-model/bamba-model.gguf"
      ],
      "metadata": {
        "id": "8DjbgjA7QUTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-quantize -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUv_HnimQmqo",
        "outputId": "1c2994d9-2a1b-4d0d-b456-3944675dca4c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: build/bin/llama-quantize [--help] [--allow-requantize] [--leave-output-tensor] [--pure] [--imatrix] [--include-weights] [--exclude-weights] [--output-tensor-type] [--token-embedding-type] [--override-kv] model-f32.gguf [model-quant.gguf] type [nthreads]\n",
            "\n",
            "  --allow-requantize: Allows requantizing tensors that have already been quantized. Warning: This can severely reduce quality compared to quantizing from 16bit or 32bit\n",
            "  --leave-output-tensor: Will leave output.weight un(re)quantized. Increases model size but may also increase quality, especially when requantizing\n",
            "  --pure: Disable k-quant mixtures and quantize all tensors to the same type\n",
            "  --imatrix file_name: use data in file_name as importance matrix for quant optimizations\n",
            "  --include-weights tensor_name: use importance matrix for this/these tensor(s)\n",
            "  --exclude-weights tensor_name: use importance matrix for this/these tensor(s)\n",
            "  --output-tensor-type ggml_type: use this ggml_type for the output.weight tensor\n",
            "  --token-embedding-type ggml_type: use this ggml_type for the token embeddings tensor\n",
            "  --keep-split: will generate quantized model in the same shards as input\n",
            "  --override-kv KEY=TYPE:VALUE\n",
            "      Advanced option to override model metadata by key in the quantized model. May be specified multiple times.\n",
            "Note: --include-weights and --exclude-weights cannot be used together\n",
            "\n",
            "Allowed quantization types:\n",
            "   2  or  Q4_0    :  4.34G, +0.4685 ppl @ Llama-3-8B\n",
            "   3  or  Q4_1    :  4.78G, +0.4511 ppl @ Llama-3-8B\n",
            "   8  or  Q5_0    :  5.21G, +0.1316 ppl @ Llama-3-8B\n",
            "   9  or  Q5_1    :  5.65G, +0.1062 ppl @ Llama-3-8B\n",
            "  19  or  IQ2_XXS :  2.06 bpw quantization\n",
            "  20  or  IQ2_XS  :  2.31 bpw quantization\n",
            "  28  or  IQ2_S   :  2.5  bpw quantization\n",
            "  29  or  IQ2_M   :  2.7  bpw quantization\n",
            "  24  or  IQ1_S   :  1.56 bpw quantization\n",
            "  31  or  IQ1_M   :  1.75 bpw quantization\n",
            "  36  or  TQ1_0   :  1.69 bpw ternarization\n",
            "  37  or  TQ2_0   :  2.06 bpw ternarization\n",
            "  10  or  Q2_K    :  2.96G, +3.5199 ppl @ Llama-3-8B\n",
            "  21  or  Q2_K_S  :  2.96G, +3.1836 ppl @ Llama-3-8B\n",
            "  23  or  IQ3_XXS :  3.06 bpw quantization\n",
            "  26  or  IQ3_S   :  3.44 bpw quantization\n",
            "  27  or  IQ3_M   :  3.66 bpw quantization mix\n",
            "  12  or  Q3_K    : alias for Q3_K_M\n",
            "  22  or  IQ3_XS  :  3.3 bpw quantization\n",
            "  11  or  Q3_K_S  :  3.41G, +1.6321 ppl @ Llama-3-8B\n",
            "  12  or  Q3_K_M  :  3.74G, +0.6569 ppl @ Llama-3-8B\n",
            "  13  or  Q3_K_L  :  4.03G, +0.5562 ppl @ Llama-3-8B\n",
            "  25  or  IQ4_NL  :  4.50 bpw non-linear quantization\n",
            "  30  or  IQ4_XS  :  4.25 bpw non-linear quantization\n",
            "  15  or  Q4_K    : alias for Q4_K_M\n",
            "  14  or  Q4_K_S  :  4.37G, +0.2689 ppl @ Llama-3-8B\n",
            "  15  or  Q4_K_M  :  4.58G, +0.1754 ppl @ Llama-3-8B\n",
            "  17  or  Q5_K    : alias for Q5_K_M\n",
            "  16  or  Q5_K_S  :  5.21G, +0.1049 ppl @ Llama-3-8B\n",
            "  17  or  Q5_K_M  :  5.33G, +0.0569 ppl @ Llama-3-8B\n",
            "  18  or  Q6_K    :  6.14G, +0.0217 ppl @ Llama-3-8B\n",
            "   7  or  Q8_0    :  7.96G, +0.0026 ppl @ Llama-3-8B\n",
            "   1  or  F16     : 14.00G, +0.0020 ppl @ Mistral-7B\n",
            "  32  or  BF16    : 14.00G, -0.0050 ppl @ Mistral-7B\n",
            "   0  or  F32     : 26.00G              @ 7B\n",
            "          COPY    : only copy tensors, no quantizing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zvlvU-HRnbZ",
        "outputId": "1313a8e6-72be-4a46-e65d-c81e50b627d5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/RichardErkhov/state-spaces_-_mamba-790m-hf-gguf/resolve/main/mamba-790m-hf.Q2_K.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4x99U5YQxq4",
        "outputId": "777102d1-51f1-4b91-b64b-6a5e3d2031ac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-19 22:14:14--  https://huggingface.co/RichardErkhov/state-spaces_-_mamba-790m-hf-gguf/resolve/main/mamba-790m-hf.Q2_K.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.55, 18.164.174.118, 18.164.174.17, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.55|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/e3/c7/e3c7ccd43d3b2ec0329e7a3e970b67a8c6592d74ea87dd5936ab224d68059c64/6cdbf6803ff2e951f08ed0a4ffc9acc644443f7cdcc83669eb7fb9bdfdbf4ea5?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mamba-790m-hf.Q2_K.gguf%3B+filename%3D%22mamba-790m-hf.Q2_K.gguf%22%3B&Expires=1740006855&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDAwNjg1NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2UzL2M3L2UzYzdjY2Q0M2QzYjJlYzAzMjllN2EzZTk3MGI2N2E4YzY1OTJkNzRlYTg3ZGQ1OTM2YWIyMjRkNjgwNTljNjQvNmNkYmY2ODAzZmYyZTk1MWYwOGVkMGE0ZmZjOWFjYzY0NDQ0M2Y3Y2RjYzgzNjY5ZWI3ZmI5YmRmZGJmNGVhNT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=c0vQk6Giphk6t%7EV4YMRDsZnw9bqUzGOh8qO7FJYj5EUVy0hayUJP24k%7E2QZOPxRthUDm28q-Ek1QnE6repKKKhhbOhiSWXQuC9PqPk0PjmHgUEERVfuz4rAHsQaPrhqUDj6pO55K%7ENPEWnLKzTvwOsUvVw4sORxClw5U0ePxQhowjBKIsCExtTBKHipIt2B%7E3MiboZT0U7BIO13GS%7E92gN7yvQb46nOt56lH893jUwoHy2jLZpv61IkRryagK6I67rodl5AeVwKjfu1P5k%7ELfKNOiIQdSlM66hINq-mzIHfw7v7biD2IDg-InYzv8QmoMRejywikM8y%7EfWfy0YSIZQ__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-02-19 22:14:15--  https://cdn-lfs-us-1.hf.co/repos/e3/c7/e3c7ccd43d3b2ec0329e7a3e970b67a8c6592d74ea87dd5936ab224d68059c64/6cdbf6803ff2e951f08ed0a4ffc9acc644443f7cdcc83669eb7fb9bdfdbf4ea5?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mamba-790m-hf.Q2_K.gguf%3B+filename%3D%22mamba-790m-hf.Q2_K.gguf%22%3B&Expires=1740006855&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDAwNjg1NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2UzL2M3L2UzYzdjY2Q0M2QzYjJlYzAzMjllN2EzZTk3MGI2N2E4YzY1OTJkNzRlYTg3ZGQ1OTM2YWIyMjRkNjgwNTljNjQvNmNkYmY2ODAzZmYyZTk1MWYwOGVkMGE0ZmZjOWFjYzY0NDQ0M2Y3Y2RjYzgzNjY5ZWI3ZmI5YmRmZGJmNGVhNT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=c0vQk6Giphk6t%7EV4YMRDsZnw9bqUzGOh8qO7FJYj5EUVy0hayUJP24k%7E2QZOPxRthUDm28q-Ek1QnE6repKKKhhbOhiSWXQuC9PqPk0PjmHgUEERVfuz4rAHsQaPrhqUDj6pO55K%7ENPEWnLKzTvwOsUvVw4sORxClw5U0ePxQhowjBKIsCExtTBKHipIt2B%7E3MiboZT0U7BIO13GS%7E92gN7yvQb46nOt56lH893jUwoHy2jLZpv61IkRryagK6I67rodl5AeVwKjfu1P5k%7ELfKNOiIQdSlM66hINq-mzIHfw7v7biD2IDg-InYzv8QmoMRejywikM8y%7EfWfy0YSIZQ__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.164.174.97, 18.164.174.19, 18.164.174.98, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.164.174.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 434089024 (414M) [binary/octet-stream]\n",
            "Saving to: ‘mamba-790m-hf.Q2_K.gguf’\n",
            "\n",
            "mamba-790m-hf.Q2_K. 100%[===================>] 413.98M  40.7MB/s    in 10s     \n",
            "\n",
            "2025-02-19 22:14:25 (40.7 MB/s) - ‘mamba-790m-hf.Q2_K.gguf’ saved [434089024/434089024]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -m /content/mamba-790m-hf.Q2_K.gguf -p \"hi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqILQiNJRw2R",
        "outputId": "64c2cd47-515b-407a-a7ae-34588ad730c1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 4358 (9177484f) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 23 key-value pairs and 482 tensors from /content/mamba-790m-hf.Q2_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = mamba\n",
            "llama_model_loader: - kv   1:                               general.name str              = mamba-790m-hf\n",
            "llama_model_loader: - kv   2:                       mamba.context_length u32              = 1048576\n",
            "llama_model_loader: - kv   3:                     mamba.embedding_length u32              = 1536\n",
            "llama_model_loader: - kv   4:                  mamba.feed_forward_length u32              = 0\n",
            "llama_model_loader: - kv   5:                 mamba.attention.head_count u32              = 0\n",
            "llama_model_loader: - kv   6:                          mamba.block_count u32              = 48\n",
            "llama_model_loader: - kv   7:                      mamba.ssm.conv_kernel u32              = 4\n",
            "llama_model_loader: - kv   8:                       mamba.ssm.inner_size u32              = 3072\n",
            "llama_model_loader: - kv   9:                       mamba.ssm.state_size u32              = 16\n",
            "llama_model_loader: - kv  10:                   mamba.ssm.time_step_rank u32              = 96\n",
            "llama_model_loader: - kv  11:     mamba.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 10\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50280]   = [\"<|endoftext|>\", \"<|padding|>\", \"!\",...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50280]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"h e\", \"i n...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  385 tensors\n",
            "llama_model_loader: - type q2_K:   96 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens cache size = 28\n",
            "llm_load_vocab: token to piece cache size = 0.2984 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = mamba\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 50280\n",
            "llm_load_print_meta: n_merges         = 50009\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 1048576\n",
            "llm_load_print_meta: n_embd           = 1536\n",
            "llm_load_print_meta: n_layer          = 48\n",
            "llm_load_print_meta: n_head           = 0\n",
            "llm_load_print_meta: n_head_kv        = 0\n",
            "llm_load_print_meta: n_rot            = 0\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 0\n",
            "llm_load_print_meta: n_embd_head_v    = 0\n",
            "llm_load_print_meta: n_gqa            = 0\n",
            "llm_load_print_meta: n_embd_k_gqa     = 0\n",
            "llm_load_print_meta: n_embd_v_gqa     = 0\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 0\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = -1\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 1048576\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 4\n",
            "llm_load_print_meta: ssm_d_inner      = 3072\n",
            "llm_load_print_meta: ssm_d_state      = 16\n",
            "llm_load_print_meta: ssm_dt_rank      = 96\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 0.8B\n",
            "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
            "llm_load_print_meta: model params     = 793.20 M\n",
            "llm_load_print_meta: model size       = 412.27 MiB (4.36 BPW) \n",
            "llm_load_print_meta: general.name     = mamba-790m-hf\n",
            "llm_load_print_meta: BOS token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: EOS token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: PAD token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: max token length = 1024\n",
            "llm_load_tensors: offloading 0 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 0/49 layers to GPU\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =   412.27 MiB\n",
            ".......................................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 4096\n",
            "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 10000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (1048576) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 1, offload = 1, type_k = 'f32', type_v = 'f32', n_layer = 48\n",
            "llama_kv_cache_init:        CPU KV buffer size =    10.69 MiB\n",
            "llama_new_context_with_model: KV self size  =   10.69 MiB, K (f32):    1.69 MiB, V (f32):    9.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   107.43 MiB\n",
            "llama_new_context_with_model: graph nodes  = 2358\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "common_init_from_params: KV cache shifting is not supported for this model (--no-context-shift to disable)'\n",
            "main: error: unable to load model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/RichardErkhov/state-spaces_-_mamba-2.8b-hf-gguf/resolve/main/mamba-2.8b-hf.Q6_K.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0Qdul1sSQUg",
        "outputId": "ed87c58d-5816-4421-e05e-876003ded503"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-19 22:17:00--  https://huggingface.co/RichardErkhov/state-spaces_-_mamba-2.8b-hf-gguf/resolve/main/mamba-2.8b-hf.Q6_K.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.17, 18.164.174.118, 18.164.174.23, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/fb/07/fb07a45ea51dbcf4dba1da1e7300da54cc263df40a44c54accee77d8f444d8ca/ede8e684c2769b8b372da2b252e28c38e6988d0f3dd088696e76e74582452c89?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mamba-2.8b-hf.Q6_K.gguf%3B+filename%3D%22mamba-2.8b-hf.Q6_K.gguf%22%3B&Expires=1740007020&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDAwNzAyMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2ZiLzA3L2ZiMDdhNDVlYTUxZGJjZjRkYmExZGExZTczMDBkYTU0Y2MyNjNkZjQwYTQ0YzU0YWNjZWU3N2Q4ZjQ0NGQ4Y2EvZWRlOGU2ODRjMjc2OWI4YjM3MmRhMmIyNTJlMjhjMzhlNjk4OGQwZjNkZDA4ODY5NmU3NmU3NDU4MjQ1MmM4OT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=I7Dr4Y0DOjxP5ilBV7Y5IWKwDGPoM-RrOsd42tTjXawyBcTT1mmuJ67mZfrqvFKkyoA3hxaPbjKqfsf4U3vLBWtVA-p3jr2Y%7E5BMkIbMo6Q%7Eu9mIwm15e05puEtHvctQ1ouZqJPeUqk0NtalU5S-4h%7EaQqkHde6rx14E9I0FauX0nizSwg1J-KWGwh0Y3NWdXn5Z9x5DJ5fy-iJa2nf2xZH5Kacid78%7EdhYYBaWZMLDHT%7Euim1TDD-Wy48pROLge5hrIjqYQqzWILapbM1qZHlWAHy%7EGMbfA3SYKxoDo5I9pz6qu1Wpckg5BYRQR5DGQfm36Ij924fTVgfOxYG5S4w__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-02-19 22:17:00--  https://cdn-lfs-us-1.hf.co/repos/fb/07/fb07a45ea51dbcf4dba1da1e7300da54cc263df40a44c54accee77d8f444d8ca/ede8e684c2769b8b372da2b252e28c38e6988d0f3dd088696e76e74582452c89?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mamba-2.8b-hf.Q6_K.gguf%3B+filename%3D%22mamba-2.8b-hf.Q6_K.gguf%22%3B&Expires=1740007020&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDAwNzAyMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2ZiLzA3L2ZiMDdhNDVlYTUxZGJjZjRkYmExZGExZTczMDBkYTU0Y2MyNjNkZjQwYTQ0YzU0YWNjZWU3N2Q4ZjQ0NGQ4Y2EvZWRlOGU2ODRjMjc2OWI4YjM3MmRhMmIyNTJlMjhjMzhlNjk4OGQwZjNkZDA4ODY5NmU3NmU3NDU4MjQ1MmM4OT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=I7Dr4Y0DOjxP5ilBV7Y5IWKwDGPoM-RrOsd42tTjXawyBcTT1mmuJ67mZfrqvFKkyoA3hxaPbjKqfsf4U3vLBWtVA-p3jr2Y%7E5BMkIbMo6Q%7Eu9mIwm15e05puEtHvctQ1ouZqJPeUqk0NtalU5S-4h%7EaQqkHde6rx14E9I0FauX0nizSwg1J-KWGwh0Y3NWdXn5Z9x5DJ5fy-iJa2nf2xZH5Kacid78%7EdhYYBaWZMLDHT%7Euim1TDD-Wy48pROLge5hrIjqYQqzWILapbM1qZHlWAHy%7EGMbfA3SYKxoDo5I9pz6qu1Wpckg5BYRQR5DGQfm36Ij924fTVgfOxYG5S4w__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.164.174.19, 18.164.174.97, 18.164.174.98, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.164.174.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2663961696 (2.5G) [binary/octet-stream]\n",
            "Saving to: ‘mamba-2.8b-hf.Q6_K.gguf’\n",
            "\n",
            "mamba-2.8b-hf.Q6_K. 100%[===================>]   2.48G  40.1MB/s    in 63s     \n",
            "\n",
            "2025-02-19 22:18:03 (40.5 MB/s) - ‘mamba-2.8b-hf.Q6_K.gguf’ saved [2663961696/2663961696]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -m /content/mamba-2.8b-hf.Q6_K.gguf -p \"hi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idk5V9WLSWiw",
        "outputId": "9c6d8cb1-4cd0-4160-b6fc-53514e266d49"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 4358 (9177484f) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 23 key-value pairs and 642 tensors from /content/mamba-2.8b-hf.Q6_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = mamba\n",
            "llama_model_loader: - kv   1:                               general.name str              = mamba-2.8b-hf\n",
            "llama_model_loader: - kv   2:                       mamba.context_length u32              = 1048576\n",
            "llama_model_loader: - kv   3:                     mamba.embedding_length u32              = 2560\n",
            "llama_model_loader: - kv   4:                  mamba.feed_forward_length u32              = 0\n",
            "llama_model_loader: - kv   5:                 mamba.attention.head_count u32              = 0\n",
            "llama_model_loader: - kv   6:                          mamba.block_count u32              = 64\n",
            "llama_model_loader: - kv   7:                      mamba.ssm.conv_kernel u32              = 4\n",
            "llama_model_loader: - kv   8:                       mamba.ssm.inner_size u32              = 5120\n",
            "llama_model_loader: - kv   9:                       mamba.ssm.state_size u32              = 16\n",
            "llama_model_loader: - kv  10:                   mamba.ssm.time_step_rank u32              = 160\n",
            "llama_model_loader: - kv  11:     mamba.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 18\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50280]   = [\"<|endoftext|>\", \"<|padding|>\", \"!\",...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50280]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"h e\", \"i n...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  513 tensors\n",
            "llama_model_loader: - type q6_K:  129 tensors\n",
            "llm_load_vocab: special tokens cache size = 28\n",
            "llm_load_vocab: token to piece cache size = 0.2984 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = mamba\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 50280\n",
            "llm_load_print_meta: n_merges         = 50009\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 1048576\n",
            "llm_load_print_meta: n_embd           = 2560\n",
            "llm_load_print_meta: n_layer          = 64\n",
            "llm_load_print_meta: n_head           = 0\n",
            "llm_load_print_meta: n_head_kv        = 0\n",
            "llm_load_print_meta: n_rot            = 0\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 0\n",
            "llm_load_print_meta: n_embd_head_v    = 0\n",
            "llm_load_print_meta: n_gqa            = 0\n",
            "llm_load_print_meta: n_embd_k_gqa     = 0\n",
            "llm_load_print_meta: n_embd_v_gqa     = 0\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 0\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = -1\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 1048576\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 4\n",
            "llm_load_print_meta: ssm_d_inner      = 5120\n",
            "llm_load_print_meta: ssm_d_state      = 16\n",
            "llm_load_print_meta: ssm_dt_rank      = 160\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = Q6_K\n",
            "llm_load_print_meta: model params     = 2.77 B\n",
            "llm_load_print_meta: model size       = 2.48 GiB (7.69 BPW) \n",
            "llm_load_print_meta: general.name     = mamba-2.8b-hf\n",
            "llm_load_print_meta: BOS token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: EOS token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: PAD token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: max token length = 1024\n",
            "llm_load_tensors: offloading 0 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 0/65 layers to GPU\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =  2538.83 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 4096\n",
            "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 10000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (1048576) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 1, offload = 1, type_k = 'f32', type_v = 'f32', n_layer = 64\n",
            "llama_kv_cache_init:        CPU KV buffer size =    23.75 MiB\n",
            "llama_new_context_with_model: KV self size  =   23.75 MiB, K (f32):    3.75 MiB, V (f32):   20.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   113.58 MiB\n",
            "llama_new_context_with_model: graph nodes  = 3142\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "common_init_from_params: KV cache shifting is not supported for this model (--no-context-shift to disable)'\n",
            "main: error: unable to load model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qh8M45PZS_qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pj6eVNNGS_n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "git clone --branch BambaArchitecture git@github.com:gabe-l-hart/llama.cpp.git\n",
        "cd llama.cpp\n",
        "mkdir build\n",
        "cd build\n",
        "# NOTE: To build with debug symbols and extra logging, use CMAKE_BUILD_TYPE=Debug\n",
        "cmake .. -DCMAKE_BUILD_TYPE=Release\n",
        "make -j"
      ],
      "metadata": {
        "id": "GFXZWdEzS_mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggml-org/llama.cpp.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKXuw4V8TLry",
        "outputId": "1549ad78-3357-4b4c-c6b5-eb3458df613d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 44372, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 44372 (delta 32), reused 19 (delta 18), pack-reused 44320 (from 2)\u001b[K\n",
            "Receiving objects: 100% (44372/44372), 91.14 MiB | 16.97 MiB/s, done.\n",
            "Resolving deltas: 100% (31978/31978), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llama.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kot3L71UTLqg",
        "outputId": "bca03689-3731-47d5-f5b0-3e49c9bfa6dc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --branch BambaArchitecture git@github.com:gabe-l-hart/llama.cpp.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKfWgtRWTCJo",
        "outputId": "a4da4e54-ccec-4c9b-e680-4f8613f200c9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "Host key verification failed.\n",
            "fatal: Could not read from remote repository.\n",
            "\n",
            "Please make sure you have the correct access rights\n",
            "and the repository exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git@github.com:ggml-org/llama.cpp.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms36itMsTbOK",
        "outputId": "adf3ebd1-5c3f-4381-b47e-9c16511f8b15"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: git@github.com:ggml-org/llama.cpp.git: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git@github.com:gabe-l-hart/llama.cpp.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_moOgWRTbqR",
        "outputId": "9679cdad-64ab-4e0e-d4fd-840046def90f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: git@github.com:gabe-l-hart/llama.cpp.git: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --branch BambaArchitecture git@github.com:gabe-l-hart/llama.cpp.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN4NrPXQTxuo",
        "outputId": "c5a1359f-243f-4fb0-bb3b-78adca530c10"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "Host key verification failed.\n",
            "fatal: Could not read from remote repository.\n",
            "\n",
            "Please make sure you have the correct access rights\n",
            "and the repository exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --branch BambaArchitecture https://github.com/gabe-l-hart/llama.cpp.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePsYMFfIUWXR",
        "outputId": "3a1042b5-d1a8-44aa-d55e-59c51e9083e0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 30216, done.\u001b[K\n",
            "remote: Total 30216 (delta 0), reused 0 (delta 0), pack-reused 30216 (from 1)\u001b[K\n",
            "Receiving objects: 100% (30216/30216), 62.55 MiB | 11.33 MiB/s, done.\n",
            "Resolving deltas: 100% (21559/21559), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llama.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg12BsRmUbp5",
        "outputId": "c5e29b7f-f5c7-4c1f-9f85-577cd8d955d4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp/llama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir build"
      ],
      "metadata": {
        "id": "H_4eAzKHUcNZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd build"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC-V1zT8Ue4Q",
        "outputId": "d445124d-1a98-4aaf-89a3-ae1908db6fc8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp/llama.cpp/build\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cmake .. -DCMAKE_BUILD_TYPE=Release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWe-KHZ5UgvM",
        "outputId": "7cb91a31-c1c6-4004-99c2-8fff9402b23c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- OpenMP found\n",
            "-- Using llamafile\n",
            "-- x86 detected\n",
            "-- Using runtime weight conversion of Q4_0 to Q4_0_x_x to enable optimized GEMM/GEMV kernels\n",
            "-- Including CPU backend\n",
            "-- Using AMX\n",
            "-- Including AMX backend\n",
            "-- Configuring done (4.1s)\n",
            "-- Generating done (0.4s)\n",
            "-- Build files have been written to: /content/llama.cpp/llama.cpp/build\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make -j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG2NlGkAUjC5",
        "outputId": "acb00214-58a9-4c5b-86ce-ef8c46e1157b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  0%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "[  7%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[  8%] Built target build_info\n",
            "[  8%] Built target sha1\n",
            "[  8%] Built target sha256\n",
            "[  8%] Built target xxhash\n",
            "[  9%] \u001b[32m\u001b[1mLinking CXX shared library libggml-base.so\u001b[0m\n",
            "[  9%] Built target ggml-base\n",
            "[  9%] \u001b[32mBuilding C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.c.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/ggml-amx/CMakeFiles/ggml-amx.dir/ggml-amx.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object ggml/src/ggml-amx/CMakeFiles/ggml-amx.dir/mmq.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-aarch64.c.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-quants.c.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32m\u001b[1mLinking CXX shared library libggml-amx.so\u001b[0m\n",
            "[ 13%] Built target ggml-amx\n",
            "[ 13%] \u001b[32m\u001b[1mLinking CXX shared library libggml-cpu.so\u001b[0m\n",
            "[ 13%] Built target ggml-cpu\n",
            "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32m\u001b[1mLinking CXX shared library libggml.so\u001b[0m\n",
            "[ 14%] Built target ggml\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 20%] Built target llama-gguf\n",
            "[ 20%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 20%] Built target llama-gguf-hash\n",
            "[ 20%] \u001b[32m\u001b[1mLinking CXX shared library libllama.so\u001b[0m\n",
            "[ 20%] Built target llama\n",
            "[ 20%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 27%] Built target test-c\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 28%] Built target llama-simple\n",
            "[ 29%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 29%] Built target llama-simple-chat\n",
            "[ 29%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize-stats\u001b[0m\n",
            "[ 29%] Built target llama-quantize-stats\n",
            "[ 29%] Built target llava\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CXX shared library libllava_shared.so\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CXX static library libllava_static.a\u001b[0m\n",
            "[ 30%] Built target llava_shared\n",
            "[ 30%] Built target llava_static\n",
            "[ 31%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 31%] Built target common\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[34m\u001b[1mGenerating completion.js.hpp\u001b[0m\n",
            "[ 58%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 59%] \u001b[34m\u001b[1mGenerating deps_daisyui.min.css.hpp\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[34m\u001b[1mGenerating deps_markdown-it.js.hpp\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[34m\u001b[1mGenerating deps_tailwindcss.js.hpp\u001b[0m\n",
            "[ 61%] \u001b[34m\u001b[1mGenerating deps_vue.esm-browser.js.hpp\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[34m\u001b[1mGenerating index.html.hpp\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 68%] Built target test-model-load-cancel\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 68%] Built target test-log\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 70%] Built target test-rope\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 71%] Built target test-autorelease\n",
            "[ 71%] Built target test-barrier\n",
            "[ 71%] Built target test-quantize-fns\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 73%] Built target llama-lookup-merge\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gbnf-validator\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[ 75%] Built target test-tokenizer-1-spm\n",
            "[ 75%] Built target llama-tokenize\n",
            "[ 75%] Built target test-tokenizer-1-bpe\n",
            "[ 75%] Built target llama-gbnf-validator\n",
            "[ 75%] Built target test-chat-template\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 75%] Built target llama-q8dot\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 76%] Built target llama-vdot\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 77%] Built target llama-eval-callback\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 78%] Built target test-grammar-parser\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 80%] Built target llama-lookup-create\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 83%] Built target llama-save-load-state\n",
            "[ 83%] Built target llama-gguf-split\n",
            "[ 83%] Built target llama-speculative-simple\n",
            "[ 83%] Built target llama-batched\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[ 85%] Built target llama-batched-bench\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[ 86%] Built target llama-gritlm\n",
            "[ 86%] Built target llama-lookup-stats\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 87%] Built target test-llama-grammar\n",
            "[ 87%] Built target llama-passkey\n",
            "[ 87%] Built target llama-lookup\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 87%] Built target llama-embedding\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 88%] Built target llama-llava-cli\n",
            "[ 88%] Built target llama-minicpmv-cli\n",
            "[ 88%] Built target test-tokenizer-0\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 89%] Built target llama-lookahead\n",
            "[ 89%] Built target test-sampling\n",
            "[ 89%] Built target test-arg-parser\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 89%] Built target llama-parallel\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 89%] Built target llama-quantize\n",
            "[ 89%] Built target test-quantize-perf\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-infill\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 91%] Built target llama-infill\n",
            "[ 91%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 91%] Built target llama-retrieval\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 92%] Built target llama-export-lora\n",
            "[ 92%] Built target llama-cli\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 94%] Built target llama-cvector-generator\n",
            "[ 94%] Built target llama-speculative\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 95%] Built target llama-imatrix\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 96%] Built target llama-perplexity\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 97%] Built target test-grammar-integration\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 98%] Built target test-backend-ops\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 99%] Built target test-json-schema-to-grammar\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 99%] Built target llama-bench\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[100%] Built target llama-server\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PlnsEqmoUmgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i8W4G5HkU47K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ulCFX0I-U49j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z9FUoP8LU5B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y0oz73Y2U5E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BCkm40bzU5Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ششششششششششششششششششش"
      ],
      "metadata": {
        "id": "BYTWZVCxVFIs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/ibm-ai-platform/Bamba-9B/blob/main/README.md"
      ],
      "metadata": {
        "id": "12hRaMeUWycY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/state-spaces"
      ],
      "metadata": {
        "id": "4uknYas3W1ks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/state-spaces/mamba?tab=readme-ov-file#installation"
      ],
      "metadata": {
        "id": "qRou9SGdW-xt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dWuscrBEW-v0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hRWdaVNkW1jI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## aشغال"
      ],
      "metadata": {
        "id": "Gi0oKhUoWij9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggml-org/llama.cpp.git\n",
        "%cd llama.cpp\n",
        "!git clone --branch BambaArchitecture git@github.com:gabe-l-hart/llama.cpp.git\n",
        "!git@github.com:ggml-org/llama.cpp.git\n",
        "!git@github.com:gabe-l-hart/llama.cpp.git\n",
        "!git clone --branch BambaArchitecture git@github.com:gabe-l-hart/llama.cpp.git\n",
        "!git clone --branch BambaArchitecture https://github.com/gabe-l-hart/llama.cpp.git\n",
        "%cd llama.cpp\n",
        "!mkdir build\n",
        "%cd build\n",
        "!cmake .. -DCMAKE_BUILD_TYPE=Release\n",
        "!make -j"
      ],
      "metadata": {
        "id": "4YEz6PGmU5ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/llama.cpp/llama.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo1Rg3IJWP_J",
        "outputId": "2a11ff99-8704-4c36-b70d-102736f4aacc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp/llama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال"
      ],
      "metadata": {
        "id": "58-hugrbWl5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -m /content/mamba-790m-hf.Q2_K.gguf -p \"hi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-PBaRrvWP8C",
        "outputId": "8b8ca308-9d73-4db1-a3a2-e195e61fbf7a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 4208 (b83e9a6c) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 23 key-value pairs and 482 tensors from /content/mamba-790m-hf.Q2_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = mamba\n",
            "llama_model_loader: - kv   1:                               general.name str              = mamba-790m-hf\n",
            "llama_model_loader: - kv   2:                       mamba.context_length u32              = 1048576\n",
            "llama_model_loader: - kv   3:                     mamba.embedding_length u32              = 1536\n",
            "llama_model_loader: - kv   4:                  mamba.feed_forward_length u32              = 0\n",
            "llama_model_loader: - kv   5:                 mamba.attention.head_count u32              = 0\n",
            "llama_model_loader: - kv   6:                          mamba.block_count u32              = 48\n",
            "llama_model_loader: - kv   7:                      mamba.ssm.conv_kernel u32              = 4\n",
            "llama_model_loader: - kv   8:                       mamba.ssm.inner_size u32              = 3072\n",
            "llama_model_loader: - kv   9:                       mamba.ssm.state_size u32              = 16\n",
            "llama_model_loader: - kv  10:                   mamba.ssm.time_step_rank u32              = 96\n",
            "llama_model_loader: - kv  11:     mamba.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 10\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50280]   = [\"<|endoftext|>\", \"<|padding|>\", \"!\",...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50280]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"h e\", \"i n...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  385 tensors\n",
            "llama_model_loader: - type q2_K:   96 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens cache size = 28\n",
            "llm_load_vocab: token to piece cache size = 0.2984 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = mamba\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 50280\n",
            "llm_load_print_meta: n_merges         = 50009\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 1048576\n",
            "llm_load_print_meta: n_embd           = 1536\n",
            "llm_load_print_meta: n_layer          = 48\n",
            "llm_load_print_meta: n_head           = 0\n",
            "llm_load_print_meta: n_head_kv        = 0\n",
            "llm_load_print_meta: n_rot            = 0\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 0\n",
            "llm_load_print_meta: n_embd_head_v    = 0\n",
            "llm_load_print_meta: n_gqa            = 0\n",
            "llm_load_print_meta: n_embd_k_gqa     = 0\n",
            "llm_load_print_meta: n_embd_v_gqa     = 0\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 0\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = -1\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 1048576\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 4\n",
            "llm_load_print_meta: ssm_d_inner      = 3072\n",
            "llm_load_print_meta: ssm_d_state      = 16\n",
            "llm_load_print_meta: ssm_dt_rank      = 96\n",
            "llm_load_print_meta: ssm_n_group      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: ssm_head_dim     = 0\n",
            "llm_load_print_meta: model type       = 0.8B\n",
            "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
            "llm_load_print_meta: model params     = 793.20 M\n",
            "llm_load_print_meta: model size       = 412.27 MiB (4.36 BPW) \n",
            "llm_load_print_meta: general.name     = mamba-790m-hf\n",
            "llm_load_print_meta: BOS token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: EOS token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: PAD token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: max token length = 1024\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =   412.27 MiB\n",
            ".......................................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 4096\n",
            "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 10000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (1048576) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init:        CPU KV buffer size =    10.69 MiB\n",
            "llama_new_context_with_model: KV self size  =   10.69 MiB, K (f32):    1.69 MiB, V (f32):    9.00 MiB\n",
            "llama_new_context_with_model: KV hybrid size  =    0.00 MiB, K (f32):    0.00 MiB, V (f32):    0.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   107.24 MiB\n",
            "llama_new_context_with_model: graph nodes  = 2598\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | \n",
            "\n",
            "sampler seed: 238200903\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 0\n",
            "\n",
            "hi’ irr’’’’’’’’cursors’cursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursors’’’’cursorsoclonal’’’’’’’’cursors’cursors irr’ irrcursors’’cursorscursors dec’’’’’cursors’’’’’’cursors deccursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursors’’’’’cursors dec’cursorscursorscursorscursorscursorscursorscursorscursorscursorscursorscursorsoclonal’cursors’ dec deccursorscursors dec decscillscillscill’’’’’’’’’scill’’scill’scill irr dec dec dec dec decscill’’’ ir decscill’ ir dec decscill dec dec dec dec dec decscill decscill decscill dec decscillscillscillscill ir decscill dec decscill decscill dec dec dec decscill decscillscill decthurner’’cursors’’’cursors’’’’’cursorscursors’cursors’ dec dec dec decscill dec dec dec decscill decscill dec decmeric’’’’’’mericcursorscursorscursorscursors dec’ dec dec dec dec dec dec dec decmericmeric dec dec decmeric dec decmericmericmericmeric dec dec dec dec dec decmeric decmeric decmericmeric decmericmericmericmeric dec decmericmericmericmericmericmeric decmericmericmericmericmericmericmericmeric decmericmericmericmericmericmericmericmeric decmericmericmericmericmericscill’scill’’’’’’’’’’scillscill’’scill dec dec dec decscillmericmeric decmericmericmericmericmericmeric decmericmericmericmericmericmericmericmeric decmericmericmericmericmericmericmeric decmericmericscill dec dec decmericmeric decscillmeric decscillmericscill decmericmericmericmericmeric dec dec dec decmeric decscillscill decmericmericmericmericmericmericmericmericmeric decscillmeric decscillscillmeric dec decmericmericmericmericmericmericmericmericmeric dec decmericmeric decmericscill decmericmericmericmericmericmericmericmericmeric decmericmericmericmericmeric decmericmericmericmericmericmeric decscill decmericmericmericmericmeric decmericscillmeric decmericmericmericmeric decmericscill decmericmericmericmericmericmeric dec dec decmeric decmericmeric decmericscillmeric decscillmericscill decscillmeric decmericmericmericmeric decmericscillmericmeric dec dec decmeric decmericmeric decmericscillmeric decmericscillmericmeric decmericscillmericmeric dec decmericmericmericmericmeric decmericmericmericmericmeric decmericmericner’’nernernernerscill’’’’nernernernernernernerscillnerscillnernerscillnernernernernernernernernerscillnerscillnernerscillscillscillnerscillnernernerscillnernernernernernernernerscillnerscillscillmericnernermericnernernernernernerscillnernernermericnermericnerscillnernernernerscillnernerscillnernernernernernernernernernernernerscillnerscillnernernerscillscillscillmericnernernerscillnernerscillnerscillnernernernernernernernernerscillnerscillnerscillnernernerscillnerscillnerscillnernernerscillnerscillnernernerscillscillscillnernerscillscillmericnernernernerscillmericnernernernernernernernerscillnernerscillnerscillmericnernerscillnernerscillnernerscillnerscillnerscillnernernerscillnerscillnerscillnernernerscillnernernernernernernernernernernerscillscillscillnerscillscillscillmericnernerscillscillnernernerscillnernerscillnerscillnerscillnerscillnerscillnernerscillmericnernerscillnernerscillmericmericnerscillmericnerscillnernernerscillnernerscillnernernernernernernernerscillscillmericnernerscillmericmericmericnerscillscillmericnerscillnernernernernernernernernernernernerscillnernerscillmericnerscillscillmericmericnerscillmericmericnerscillmericnerscillmericnerscillmericnerscillmericnerscillmericnerscillmericmericmericmericnerscillscillmericnernernernerscillnernernerscillmericnernerscillscillmericnerscillscillmericmericmericmericmericnerscillscillmericnernerner낿’’’’’’nernernernernernernernernerscillnernernernernernernernernernernerscillnernernernernernernernerscillscillnernernernerscillnerscillnernernernernernernernernernernerscillnernernernernernernernerner�\n",
            "\n",
            " ’’’’’’’’’nerscillscillnernernernernernernernernernernernernernernernernernernernernernernernernernernernernerscillnernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernerner�\n",
            "\n",
            " ’oclonal’’’’’’’’’’’’’cursors’’’cursorsistorsistors’’’’’’’’’’’’’’’’cursors’’’’’’cursors’’nernernernernernernerscillscillnernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernerner�\n",
            "\n",
            " ’’’’’nernerner’scill’’’’’nernerscill’nernerscill’nernernernernernernernernernernernernernernernernerner낿’’’낿’’’’’’’’’’’’’’’’’’’cursorsnertypennerner’’ner’cursorsner’’’’ner’’’grily’’’’’’cursorsner’’’’’’’’’’’cursorsnerner’nernernernernernernerscill’’’’ner’낿’’’’’�\n",
            "\n",
            " ’’’’’mmrner’’’’’’’’’’’’’’ner’’’’’’’’’scillner’’’’mmrnerscillner’’’’’’’’’’’’ner’’’’’’’’’’’’’ner’’’’nermmrnerner’ner’nerner’’’’’’nerner’’’’’’nermmr’nernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernerner�\n",
            "\n",
            " istorsnerner’nernerner’’’’’’nerner’’’’mmrner’nermmrnernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernernerner�\n",
            "\n",
            " ’’ner’낿’’’’’’� ’’’’’’nerner’’’’’mmrnernernernernerner� ’’mmrnernernernernernernernernernernernernernernernerner� \n",
            "\n",
            " ’’’’’’’’nerner’’’scill’’’’’’’’’’nernernernernernernernernernernernernerner\n",
            "llama_perf_sampler_print:    sampling time =     205.84 ms /  2175 runs   (    0.09 ms per token, 10566.72 tokens per second)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -m /content/mamba-790m-hf.Q2_K.gguf -p \"hi\""
      ],
      "metadata": {
        "id": "4G462eiHWrPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./bin/llama-cli  -ngl 0 -m /content/mamba-2.8b-hf.Q6_K.gguf -p \"Tell me a story about a developer and their dog\""
      ],
      "metadata": {
        "id": "PL4YxvCBXJtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY-C98p5XcZ6",
        "outputId": "373acd4b-00a9-424e-99ce-33a728cff64f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- common params -----\n",
            "\n",
            "-h,    --help, --usage                  print usage and exit\n",
            "--version                               show version and build info\n",
            "--verbose-prompt                        print a verbose prompt before generation (default: false)\n",
            "-t,    --threads N                      number of threads to use during generation (default: -1)\n",
            "                                        (env: LLAMA_ARG_THREADS)\n",
            "-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:\n",
            "                                        same as --threads)\n",
            "-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range\n",
            "                                        (default: \"\")\n",
            "-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask\n",
            "--cpu-strict <0|1>                      use strict CPU placement (default: 0)\n",
            "--prio N                                set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)\n",
            "-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch\n",
            "                                        (default: same as --cpu-mask)\n",
            "-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch\n",
            "--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)\n",
            "--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)\n",
            "-c,    --ctx-size N                     size of the prompt context (default: 4096, 0 = loaded from model)\n",
            "                                        (env: LLAMA_ARG_CTX_SIZE)\n",
            "-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity, -2 = until\n",
            "                                        context filled)\n",
            "                                        (env: LLAMA_ARG_N_PREDICT)\n",
            "-b,    --batch-size N                   logical maximum batch size (default: 2048)\n",
            "                                        (env: LLAMA_ARG_BATCH)\n",
            "-ub,   --ubatch-size N                  physical maximum batch size (default: 512)\n",
            "                                        (env: LLAMA_ARG_UBATCH)\n",
            "--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =\n",
            "                                        all)\n",
            "-fa,   --flash-attn                     enable Flash Attention (default: disabled)\n",
            "                                        (env: LLAMA_ARG_FLASH_ATTN)\n",
            "-p,    --prompt PROMPT                  prompt to start generation with\n",
            "                                        if -cnv is set, this will be used as system prompt\n",
            "--no-perf                               disable internal libllama performance timings (default: false)\n",
            "                                        (env: LLAMA_ARG_NO_PERF)\n",
            "-f,    --file FNAME                     a file containing the prompt (default: none)\n",
            "-bf,   --binary-file FNAME              binary file containing the prompt (default: none)\n",
            "-e,    --escape                         process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\) (default: true)\n",
            "--no-escape                             do not process escape sequences\n",
            "--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by\n",
            "                                        the model\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)\n",
            "--rope-scale N                          RoPE context scaling factor, expands context by a factor of N\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALE)\n",
            "--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from\n",
            "                                        model)\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)\n",
            "--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)\n",
            "--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training\n",
            "                                        context size)\n",
            "                                        (env: LLAMA_ARG_YARN_ORIG_CTX)\n",
            "--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full\n",
            "                                        interpolation)\n",
            "                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)\n",
            "--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)\n",
            "--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_SLOW)\n",
            "--yarn-beta-fast N                      YaRN: low correction dim or beta (default: 32.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_FAST)\n",
            "-dkvc, --dump-kv-cache                  verbose print of the KV cache\n",
            "-nkvo, --no-kv-offload                  disable KV offload\n",
            "                                        (env: LLAMA_ARG_NO_KV_OFFLOAD)\n",
            "-ctk,  --cache-type-k TYPE              KV cache data type for K (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K)\n",
            "-ctv,  --cache-type-v TYPE              KV cache data type for V (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V)\n",
            "-dt,   --defrag-thold N                 KV cache defragmentation threshold (default: 0.1, < 0 - disabled)\n",
            "                                        (env: LLAMA_ARG_DEFRAG_THOLD)\n",
            "-np,   --parallel N                     number of parallel sequences to decode (default: 1)\n",
            "                                        (env: LLAMA_ARG_N_PARALLEL)\n",
            "--mlock                                 force system to keep model in RAM rather than swapping or compressing\n",
            "                                        (env: LLAMA_ARG_MLOCK)\n",
            "--no-mmap                               do not memory-map model (slower load but may reduce pageouts if not\n",
            "                                        using mlock)\n",
            "                                        (env: LLAMA_ARG_NO_MMAP)\n",
            "--numa TYPE                             attempt optimizations that help on some NUMA systems\n",
            "                                        - distribute: spread execution evenly over all nodes\n",
            "                                        - isolate: only spawn threads on CPUs on the node that execution\n",
            "                                        started on\n",
            "                                        - numactl: use the CPU map provided by numactl\n",
            "                                        if run without this previously, it is recommended to drop the system\n",
            "                                        page cache before using this\n",
            "                                        see https://github.com/ggerganov/llama.cpp/issues/1437\n",
            "                                        (env: LLAMA_ARG_NUMA)\n",
            "-ngl,  --gpu-layers, --n-gpu-layers N   number of layers to store in VRAM\n",
            "                                        (env: LLAMA_ARG_N_GPU_LAYERS)\n",
            "-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\n",
            "                                        - none: use one GPU only\n",
            "                                        - layer (default): split layers and KV across GPUs\n",
            "                                        - row: split rows across GPUs\n",
            "                                        (env: LLAMA_ARG_SPLIT_MODE)\n",
            "-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of\n",
            "                                        proportions, e.g. 3,1\n",
            "                                        (env: LLAMA_ARG_TENSOR_SPLIT)\n",
            "-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for\n",
            "                                        intermediate results and KV (with split-mode = row) (default: 0)\n",
            "                                        (env: LLAMA_ARG_MAIN_GPU)\n",
            "--check-tensors                         check model tensor data for invalid values (default: false)\n",
            "--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified\n",
            "                                        multiple times.\n",
            "                                        types: int, float, bool, str. example: --override-kv\n",
            "                                        tokenizer.ggml.add_bos_token=bool:false\n",
            "--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)\n",
            "--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use\n",
            "                                        multiple adapters)\n",
            "--control-vector FNAME                  add a control vector\n",
            "                                        note: this argument can be repeated to add multiple control vectors\n",
            "--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE\n",
            "                                        note: this argument can be repeated to add multiple scaled control\n",
            "                                        vectors\n",
            "--control-vector-layer-range START END\n",
            "                                        layer range to apply the control vector(s) to, start and end inclusive\n",
            "-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`\n",
            "                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)\n",
            "                                        (env: LLAMA_ARG_MODEL)\n",
            "-mu,   --model-url MODEL_URL            model download url (default: unused)\n",
            "                                        (env: LLAMA_ARG_MODEL_URL)\n",
            "-hfr,  --hf-repo REPO                   Hugging Face model repository (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO)\n",
            "-hff,  --hf-file FILE                   Hugging Face model file (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE)\n",
            "-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment\n",
            "                                        variable)\n",
            "                                        (env: HF_TOKEN)\n",
            "--log-disable                           Log disable\n",
            "--log-file FNAME                        Log to file\n",
            "--log-colors                            Enable colored logging\n",
            "                                        (env: LLAMA_LOG_COLORS)\n",
            "-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for\n",
            "                                        debugging)\n",
            "-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be\n",
            "                                        ignored.\n",
            "                                        (env: LLAMA_LOG_VERBOSITY)\n",
            "--log-prefix                            Enable prefx in log messages\n",
            "                                        (env: LLAMA_LOG_PREFIX)\n",
            "--log-timestamps                        Enable timestamps in log messages\n",
            "                                        (env: LLAMA_LOG_TIMESTAMPS)\n",
            "\n",
            "\n",
            "----- sampling params -----\n",
            "\n",
            "--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by\n",
            "                                        ';'\n",
            "                                        (default: dry;top_k;typ_p;top_p;min_p;xtc;temperature)\n",
            "-s,    --seed SEED                      RNG seed (default: -1, use random seed for -1)\n",
            "--sampling-seq SEQUENCE                 simplified sequence for samplers that will be used (default: dkypmxt)\n",
            "--ignore-eos                            ignore end of stream token and continue generating (implies\n",
            "                                        --logit-bias EOS-inf)\n",
            "--penalize-nl                           penalize newline tokens (default: false)\n",
            "--temp N                                temperature (default: 0.8)\n",
            "--top-k N                               top-k sampling (default: 40, 0 = disabled)\n",
            "--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)\n",
            "--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)\n",
            "--xtc-probability N                     xtc probability (default: 0.0, 0.0 = disabled)\n",
            "--xtc-threshold N                       xtc threshold (default: 0.1, 1.0 = disabled)\n",
            "--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
            "--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1\n",
            "                                        = ctx_size)\n",
            "--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
            "--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
            "--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
            "--dry-multiplier N                      set DRY sampling multiplier (default: 0.0, 0.0 = disabled)\n",
            "--dry-base N                            set DRY sampling base value (default: 1.75)\n",
            "--dry-allowed-length N                  set allowed length for DRY sampling (default: 2)\n",
            "--dry-penalty-last-n N                  set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 =\n",
            "                                        context size)\n",
            "--dry-sequence-breaker STRING           add sequence breaker for DRY sampling, clearing out default breakers\n",
            "                                        ('\\n', ':', '\"', '*') in the process; use \"none\" to not use any\n",
            "                                        sequence breakers\n",
            "--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
            "--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)\n",
            "--mirostat N                            use Mirostat sampling.\n",
            "                                        Top K, Nucleus and Locally Typical samplers are ignored if used.\n",
            "                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
            "--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)\n",
            "--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)\n",
            "-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,\n",
            "                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
            "                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
            "--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/\n",
            "                                        dir) (default: '')\n",
            "--grammar-file FNAME                    file to read grammar from\n",
            "-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.\n",
            "                                        `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "\n",
            "\n",
            "----- example-specific params -----\n",
            "\n",
            "--no-display-prompt                     don't print prompt at generation (default: false)\n",
            "-co,   --color                          colorise output to distinguish prompt and user input from generations\n",
            "                                        (default: false)\n",
            "--no-context-shift                      disables context shift on inifinite text generation (default:\n",
            "                                        disabled)\n",
            "                                        (env: LLAMA_ARG_NO_CONTEXT_SHIFT)\n",
            "-ptc,  --print-token-count N            print token count every N tokens (default: -1)\n",
            "--prompt-cache FNAME                    file to cache prompt state for faster startup (default: none)\n",
            "--prompt-cache-all                      if specified, saves user input and generations to cache as well\n",
            "--prompt-cache-ro                       if specified, uses the prompt cache but does not update it\n",
            "-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode\n",
            "-sp,   --special                        special tokens output enabled (default: false)\n",
            "-cnv,  --conversation                   run in conversation mode:\n",
            "                                        - does not print special tokens and suffix/prefix\n",
            "                                        - interactive mode is also enabled\n",
            "                                        (default: false)\n",
            "-i,    --interactive                    run in interactive mode (default: false)\n",
            "-if,   --interactive-first              run in interactive mode and wait for input right away (default: false)\n",
            "-mli,  --multiline-input                allows you to write or paste multiple lines without ending each in '\\'\n",
            "--in-prefix-bos                         prefix BOS to user inputs, preceding the `--in-prefix` string\n",
            "--in-prefix STRING                      string to prefix user inputs with (default: empty)\n",
            "--in-suffix STRING                      string to suffix after user inputs with (default: empty)\n",
            "--no-warmup                             skip warming up the model with an empty run\n",
            "-gan,  --grp-attn-n N                   group-attention factor (default: 1)\n",
            "                                        (env: LLAMA_ARG_GRP_ATTN_N)\n",
            "-gaw,  --grp-attn-w N                   group-attention width (default: 512)\n",
            "                                        (env: LLAMA_ARG_GRP_ATTN_W)\n",
            "--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's\n",
            "                                        metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted:\n",
            "                                        https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE)\n",
            "--simple-io                             use basic IO for better compatibility in subprocesses and limited\n",
            "                                        consoles\n",
            "\n",
            "example usage:\n",
            "\n",
            "  text generation:     build/bin/llama-cli -m your_model.gguf -p \"I believe the meaning of life is\" -n 128\n",
            "\n",
            "  chat (conversation): build/bin/llama-cli -m your_model.gguf -p \"You are a helpful assistant\" -cnv\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lHPkUoLXXcYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----- common params -----\n",
        "\n",
        "-h,    --help, --usage                  print usage and exit\n",
        "--version                               show version and build info\n",
        "--verbose-prompt                        print a verbose prompt before generation (default: false)\n",
        "-t,    --threads N                      number of threads to use during generation (default: -1)\n",
        "                                        (env: LLAMA_ARG_THREADS)\n",
        "-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:\n",
        "                                        same as --threads)\n",
        "-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range\n",
        "                                        (default: \"\")\n",
        "-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask\n",
        "--cpu-strict <0|1>                      use strict CPU placement (default: 0)\n",
        "--prio N                                set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
        "                                        (default: 0)\n",
        "--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)\n",
        "-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch\n",
        "                                        (default: same as --cpu-mask)\n",
        "-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch\n",
        "--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)\n",
        "--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
        "                                        (default: 0)\n",
        "--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)\n",
        "-c,    --ctx-size N                     size of the prompt context (default: 4096, 0 = loaded from model)\n",
        "                                        (env: LLAMA_ARG_CTX_SIZE)\n",
        "-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity, -2 = until\n",
        "                                        context filled)\n",
        "                                        (env: LLAMA_ARG_N_PREDICT)\n",
        "-b,    --batch-size N                   logical maximum batch size (default: 2048)\n",
        "                                        (env: LLAMA_ARG_BATCH)\n",
        "-ub,   --ubatch-size N                  physical maximum batch size (default: 512)\n",
        "                                        (env: LLAMA_ARG_UBATCH)\n",
        "--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =\n",
        "                                        all)\n",
        "-fa,   --flash-attn                     enable Flash Attention (default: disabled)\n",
        "                                        (env: LLAMA_ARG_FLASH_ATTN)\n",
        "-p,    --prompt PROMPT                  prompt to start generation with\n",
        "                                        if -cnv is set, this will be used as system prompt\n",
        "--no-perf                               disable internal libllama performance timings (default: false)\n",
        "                                        (env: LLAMA_ARG_NO_PERF)\n",
        "-f,    --file FNAME                     a file containing the prompt (default: none)\n",
        "-bf,   --binary-file FNAME              binary file containing the prompt (default: none)\n",
        "-e,    --escape                         process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\) (default: true)\n",
        "--no-escape                             do not process escape sequences\n",
        "--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by\n",
        "                                        the model\n",
        "                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)\n",
        "--rope-scale N                          RoPE context scaling factor, expands context by a factor of N\n",
        "                                        (env: LLAMA_ARG_ROPE_SCALE)\n",
        "--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from\n",
        "                                        model)\n",
        "                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)\n",
        "--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N\n",
        "                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)\n",
        "--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training\n",
        "                                        context size)\n",
        "                                        (env: LLAMA_ARG_YARN_ORIG_CTX)\n",
        "--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full\n",
        "                                        interpolation)\n",
        "                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)\n",
        "--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n",
        "                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)\n",
        "--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: 1.0)\n",
        "                                        (env: LLAMA_ARG_YARN_BETA_SLOW)\n",
        "--yarn-beta-fast N                      YaRN: low correction dim or beta (default: 32.0)\n",
        "                                        (env: LLAMA_ARG_YARN_BETA_FAST)\n",
        "-dkvc, --dump-kv-cache                  verbose print of the KV cache\n",
        "-nkvo, --no-kv-offload                  disable KV offload\n",
        "                                        (env: LLAMA_ARG_NO_KV_OFFLOAD)\n",
        "-ctk,  --cache-type-k TYPE              KV cache data type for K (default: f16)\n",
        "                                        (env: LLAMA_ARG_CACHE_TYPE_K)\n",
        "-ctv,  --cache-type-v TYPE              KV cache data type for V (default: f16)\n",
        "                                        (env: LLAMA_ARG_CACHE_TYPE_V)\n",
        "-dt,   --defrag-thold N                 KV cache defragmentation threshold (default: 0.1, < 0 - disabled)\n",
        "                                        (env: LLAMA_ARG_DEFRAG_THOLD)\n",
        "-np,   --parallel N                     number of parallel sequences to decode (default: 1)\n",
        "                                        (env: LLAMA_ARG_N_PARALLEL)\n",
        "--mlock                                 force system to keep model in RAM rather than swapping or compressing\n",
        "                                        (env: LLAMA_ARG_MLOCK)\n",
        "--no-mmap                               do not memory-map model (slower load but may reduce pageouts if not\n",
        "                                        using mlock)\n",
        "                                        (env: LLAMA_ARG_NO_MMAP)\n",
        "--numa TYPE                             attempt optimizations that help on some NUMA systems\n",
        "                                        - distribute: spread execution evenly over all nodes\n",
        "                                        - isolate: only spawn threads on CPUs on the node that execution\n",
        "                                        started on\n",
        "                                        - numactl: use the CPU map provided by numactl\n",
        "                                        if run without this previously, it is recommended to drop the system\n",
        "                                        page cache before using this\n",
        "                                        see https://github.com/ggerganov/llama.cpp/issues/1437\n",
        "                                        (env: LLAMA_ARG_NUMA)\n",
        "-ngl,  --gpu-layers, --n-gpu-layers N   number of layers to store in VRAM\n",
        "                                        (env: LLAMA_ARG_N_GPU_LAYERS)\n",
        "-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\n",
        "                                        - none: use one GPU only\n",
        "                                        - layer (default): split layers and KV across GPUs\n",
        "                                        - row: split rows across GPUs\n",
        "                                        (env: LLAMA_ARG_SPLIT_MODE)\n",
        "-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of\n",
        "                                        proportions, e.g. 3,1\n",
        "                                        (env: LLAMA_ARG_TENSOR_SPLIT)\n",
        "-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for\n",
        "                                        intermediate results and KV (with split-mode = row) (default: 0)\n",
        "                                        (env: LLAMA_ARG_MAIN_GPU)\n",
        "--check-tensors                         check model tensor data for invalid values (default: false)\n",
        "--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified\n",
        "                                        multiple times.\n",
        "                                        types: int, float, bool, str. example: --override-kv\n",
        "                                        tokenizer.ggml.add_bos_token=bool:false\n",
        "--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)\n",
        "--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use\n",
        "                                        multiple adapters)\n",
        "--control-vector FNAME                  add a control vector\n",
        "                                        note: this argument can be repeated to add multiple control vectors\n",
        "--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE\n",
        "                                        note: this argument can be repeated to add multiple scaled control\n",
        "                                        vectors\n",
        "--control-vector-layer-range START END\n",
        "                                        layer range to apply the control vector(s) to, start and end inclusive\n",
        "-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`\n",
        "                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)\n",
        "                                        (env: LLAMA_ARG_MODEL)\n",
        "-mu,   --model-url MODEL_URL            model download url (default: unused)\n",
        "                                        (env: LLAMA_ARG_MODEL_URL)\n",
        "-hfr,  --hf-repo REPO                   Hugging Face model repository (default: unused)\n",
        "                                        (env: LLAMA_ARG_HF_REPO)\n",
        "-hff,  --hf-file FILE                   Hugging Face model file (default: unused)\n",
        "                                        (env: LLAMA_ARG_HF_FILE)\n",
        "-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment\n",
        "                                        variable)\n",
        "                                        (env: HF_TOKEN)\n",
        "--log-disable                           Log disable\n",
        "--log-file FNAME                        Log to file\n",
        "--log-colors                            Enable colored logging\n",
        "                                        (env: LLAMA_LOG_COLORS)\n",
        "-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for\n",
        "                                        debugging)\n",
        "-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be\n",
        "                                        ignored.\n",
        "                                        (env: LLAMA_LOG_VERBOSITY)\n",
        "--log-prefix                            Enable prefx in log messages\n",
        "                                        (env: LLAMA_LOG_PREFIX)\n",
        "--log-timestamps                        Enable timestamps in log messages\n",
        "                                        (env: LLAMA_LOG_TIMESTAMPS)\n",
        "\n",
        "\n",
        "----- sampling params -----\n",
        "\n",
        "--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by\n",
        "                                        ';'\n",
        "                                        (default: dry;top_k;typ_p;top_p;min_p;xtc;temperature)\n",
        "-s,    --seed SEED                      RNG seed (default: -1, use random seed for -1)\n",
        "--sampling-seq SEQUENCE                 simplified sequence for samplers that will be used (default: dkypmxt)\n",
        "--ignore-eos                            ignore end of stream token and continue generating (implies\n",
        "                                        --logit-bias EOS-inf)\n",
        "--penalize-nl                           penalize newline tokens (default: false)\n",
        "--temp N                                temperature (default: 0.8)\n",
        "--top-k N                               top-k sampling (default: 40, 0 = disabled)\n",
        "--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)\n",
        "--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)\n",
        "--xtc-probability N                     xtc probability (default: 0.0, 0.0 = disabled)\n",
        "--xtc-threshold N                       xtc threshold (default: 0.1, 1.0 = disabled)\n",
        "--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
        "--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1\n",
        "                                        = ctx_size)\n",
        "--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
        "--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
        "--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
        "--dry-multiplier N                      set DRY sampling multiplier (default: 0.0, 0.0 = disabled)\n",
        "--dry-base N                            set DRY sampling base value (default: 1.75)\n",
        "--dry-allowed-length N                  set allowed length for DRY sampling (default: 2)\n",
        "--dry-penalty-last-n N                  set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 =\n",
        "                                        context size)\n",
        "--dry-sequence-breaker STRING           add sequence breaker for DRY sampling, clearing out default breakers\n",
        "                                        ('\\n', ':', '\"', '*') in the process; use \"none\" to not use any\n",
        "                                        sequence breakers\n",
        "--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
        "--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)\n",
        "--mirostat N                            use Mirostat sampling.\n",
        "                                        Top K, Nucleus and Locally Typical samplers are ignored if used.\n",
        "                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
        "--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)\n",
        "--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)\n",
        "-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,\n",
        "                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
        "                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
        "--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/\n",
        "                                        dir) (default: '')\n",
        "--grammar-file FNAME                    file to read grammar from\n",
        "-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.\n",
        "                                        `{}` for any JSON object\n",
        "                                        For schemas w/ external $refs, use --grammar +\n",
        "                                        example/json_schema_to_grammar.py instead\n",
        "\n",
        "\n",
        "----- example-specific params -----\n",
        "\n",
        "--no-display-prompt                     don't print prompt at generation (default: false)\n",
        "-co,   --color                          colorise output to distinguish prompt and user input from generations\n",
        "                                        (default: false)\n",
        "--no-context-shift                      disables context shift on inifinite text generation (default:\n",
        "                                        disabled)\n",
        "                                        (env: LLAMA_ARG_NO_CONTEXT_SHIFT)\n",
        "-ptc,  --print-token-count N            print token count every N tokens (default: -1)\n",
        "--prompt-cache FNAME                    file to cache prompt state for faster startup (default: none)\n",
        "--prompt-cache-all                      if specified, saves user input and generations to cache as well\n",
        "--prompt-cache-ro                       if specified, uses the prompt cache but does not update it\n",
        "-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode\n",
        "-sp,   --special                        special tokens output enabled (default: false)\n",
        "-cnv,  --conversation                   run in conversation mode:\n",
        "                                        - does not print special tokens and suffix/prefix\n",
        "                                        - interactive mode is also enabled\n",
        "                                        (default: false)\n",
        "-i,    --interactive                    run in interactive mode (default: false)\n",
        "-if,   --interactive-first              run in interactive mode and wait for input right away (default: false)\n",
        "-mli,  --multiline-input                allows you to write or paste multiple lines without ending each in '\\'\n",
        "--in-prefix-bos                         prefix BOS to user inputs, preceding the `--in-prefix` string\n",
        "--in-prefix STRING                      string to prefix user inputs with (default: empty)\n",
        "--in-suffix STRING                      string to suffix after user inputs with (default: empty)\n",
        "--no-warmup                             skip warming up the model with an empty run\n",
        "-gan,  --grp-attn-n N                   group-attention factor (default: 1)\n",
        "                                        (env: LLAMA_ARG_GRP_ATTN_N)\n",
        "-gaw,  --grp-attn-w N                   group-attention width (default: 512)\n",
        "                                        (env: LLAMA_ARG_GRP_ATTN_W)\n",
        "--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's\n",
        "                                        metadata)\n",
        "                                        if suffix/prefix are specified, template will be disabled\n",
        "                                        only commonly used templates are accepted:\n",
        "                                        https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template\n",
        "                                        (env: LLAMA_ARG_CHAT_TEMPLATE)\n",
        "--simple-io                             use basic IO for better compatibility in subprocesses and limited\n",
        "                                        consoles\n",
        "\n",
        "example usage:\n",
        "\n",
        "  text generation:     build/bin/llama-cli -m your_model.gguf -p \"I believe the meaning of life is\" -n 128\n",
        "\n",
        "  chat (conversation): build/bin/llama-cli -m your_model.gguf -p \"You are a helpful assistant\" -cnv\n"
      ],
      "metadata": {
        "id": "HxdDgpLeX1ei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----- معلمات مشتركة -----\n",
        "\n",
        "-h، --help، --usage طباعة الاستخدام والخروج\n",
        "--version عرض الإصدار ومعلومات البناء\n",
        "--verbose-prompt طباعة موجه مطوّل قبل التوليد (الإعداد الافتراضي: false)\n",
        "-t، --threads عدد N من الخيوط المستخدمة أثناء التوليد (الإعداد الافتراضي: -1)\n",
        "(البيئة: LLAMA_ARG_THREADS)\n",
        "-tb، --threads-batch عدد N من الخيوط المستخدمة أثناء المعالجة الدفعية والموجِّهة (الإعداد الافتراضي:\n",
        "نفس --threads)\n",
        "-C، --cpu-mask M قناع تقارب وحدة المعالجة المركزية: سداسي عشري طويل بشكل عشوائي. مكمِّلات نطاق وحدة المعالجة المركزية\n",
        "(الإعداد الافتراضي: \"\")\n",
        "-Cr، --cpu-range نطاق وحدات المعالجة المركزية المنخفض والعالي للتقارب. المكملات --cpu-mask\n",
        "--cpu-strict <0|1> تستخدم وضع وحدة المعالجة المركزية الصارم (افتراضي: 0)\n",
        "--prio N تعيين أولوية العملية/الخيط: 0-عادي، 1-متوسط، 2-عالي، 3-وقت حقيقي\n",
        "(افتراضي: 0)\n",
        "--poll <0...100> تستخدم مستوى الاستطلاع لانتظار العمل (0 - بدون استطلاع، افتراضي: 50)\n",
        "-Cb، --cpu-mask-batch M قناع تقارب وحدة المعالجة المركزية: سداسي عشري طويل بشكل عشوائي. المكملات cpu-range-batch\n",
        "(افتراضي: نفس --cpu-mask)\n",
        "-Crb، --cpu-range-batch نطاقات منخفضة إلى عالية لوحدات المعالجة المركزية للتقارب. المكملات --cpu-mask-batch\n",
        "--cpu-strict-batch <0|1> تستخدم وضع وحدة المعالجة المركزية الصارم (الافتراضي: نفس --cpu-strict)\n",
        "--prio-batch N تعيين أولوية العملية/الخيط: 0-عادي، 1-متوسط، 2-عالي، 3-وقت حقيقي\n",
        "(الافتراضي: 0)\n",
        "--poll-batch <0|1> تستخدم الاستطلاع لانتظار العمل (الافتراضي: نفس --poll)\n",
        "-c, --ctx-size N حجم سياق المطالبة (الافتراضي: 4096، 0 = محمل من النموذج)\n",
        "(env: LLAMA_ARG_CTX_SIZE)\n",
        "-n, --predict, --n-predict N عدد الرموز المراد التنبؤ بها (الافتراضي: -1, -1 = لانهائي، -2 = حتى\n",
        "\n",
        "امتلاء السياق)\n",
        "(env: LLAMA_ARG_N_PREDICT)\n",
        "-b, --batch-size N الحد الأقصى المنطقي حجم الدفعة (الافتراضي: 2048)\n",
        "(البيئة: LLAMA_ARG_BATCH)\n",
        "-ub، --ubatch-size الحد الأقصى الفعلي لحجم الدفعة (الافتراضي: 512)\n",
        "(البيئة: LLAMA_ARG_UBATCH)\n",
        "--احتفظ بعدد N من الرموز التي يجب الاحتفاظ بها من المطالبة الأولية (الافتراضي: 0، -1 =\n",
        "all)\n",
        "-fa، --flash-attn تمكين Flash Attention (الافتراضي: معطل)\n",
        "(البيئة: LLAMA_ARG_FLASH_ATTN)\n",
        "-p، --prompt مطالبة PROMPT لبدء التوليد\n",
        "إذا تم تعيين -cnv، فسيتم استخدام هذا كمطالبة نظام\n",
        "--no-perf تعطيل توقيتات أداء libllama الداخلية (الافتراضي: false)\n",
        "(البيئة: LLAMA_ARG_NO_PERF)\n",
        "-f، --file FNAME ملف يحتوي على المطالبة (الافتراضي: لا شيء)\n",
        "-bf، --binary-file FNAME ملف ثنائي يحتوي على المطالبة (الافتراضي: لا شيء)\n",
        "-e، --escape process escapes sequences (\\n، \\r، \\t، \\'، \\\", \\\\) (افتراضي: true)\n",
        "--no-escape do not process escape sequences\n",
        "--rope-scaling {none,linear,yarn} طريقة قياس تردد RoPE، تكون افتراضيًا خطية ما لم يتم تحديدها بواسطة\n",
        "النموذج\n",
        "(env: LLAMA_ARG_ROPE_SCALING_TYPE)\n",
        "--rope-scale N عامل قياس سياق RoPE، يوسع السياق بعامل N\n",
        "(env: LLAMA_ARG_ROPE_SCALE)\n",
        "--rope-freq-base N تردد قاعدة RoPE، المستخدم بواسطة القياس المتوافق مع NTK (افتراضي: محمل من\n",
        "النموذج)\n",
        "(env: LLAMA_ARG_ROPE_FREQ_BASE)\n",
        "--rope-freq-scale N عامل قياس تردد RoPE، يوسع السياق بعامل 1/N\n",
        "(env: LLAMA_ARG_ROPE_FREQ_SCALE)\n",
        "--yarn-orig-ctx N YaRN: حجم السياق الأصلي للنموذج (الافتراضي: 0 = حجم سياق تدريب النموذج)\n",
        "(env: LLAMA_ARG_YARN_ORIG_CTX)\n",
        "--yarn-ext-factor N YaRN: عامل مزيج الاستقراء (الافتراضي: -1.0، 0.0 = الاستيفاء الكامل)\n",
        "(env: LLAMA_ARG_YARN_EXT_FACTOR)\n",
        "--yarn-attn-factor N YaRN: مقياس sqrt(t) أو حجم الانتباه (الافتراضي: 1.0)\n",
        "(env: LLAMA_ARG_YARN_ATTN_FACTOR)\n",
        "--yarn-beta-slow N YaRN: بُعد التصحيح العالي"
      ],
      "metadata": {
        "id": "gc8THMMWX5tP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "أو ألفا (افتراضي: 1.0)\n",
        "(env: LLAMA_ARG_YARN_BETA_SLOW)\n",
        "--yarn-beta-fast N YaRN: تصحيح منخفض dim أو بيتا (افتراضي: 32.0)\n",
        "(env: LLAMA_ARG_YARN_BETA_FAST)\n",
        "-dkvc، --dump-kv-cache طباعة مطولة لذاكرة التخزين المؤقت KV\n",
        "-nkvo، --no-kv-offload تعطيل تفريغ KV\n",
        "(env: LLAMA_ARG_NO_KV_OFFLOAD)\n",
        "-ctk، --cache-type-k النوع نوع بيانات ذاكرة التخزين المؤقت KV لـ K (افتراضي: f16)\n",
        "(env: LLAMA_ARG_CACHE_TYPE_K)\n",
        "-ctv، --cache-type-v النوع نوع بيانات ذاكرة التخزين المؤقت KV لـ V (افتراضي: f16)\n",
        "(env: LLAMA_ARG_CACHE_TYPE_V)\n",
        "-dt، --defrag-thold حد إلغاء تجزئة ذاكرة التخزين المؤقت N KV (افتراضي: 0.1، < 0 - معطل)\n",
        "(env: LLAMA_ARG_DEFRAG_THOLD)\n",
        "-np، --parallel عدد N من التسلسلات المتوازية لفك تشفيرها (افتراضي: 1)\n",
        "(env: LLAMA_ARG_N_PARALLEL)\n",
        "--mlock إجبار النظام على الاحتفاظ بالنموذج في ذاكرة الوصول العشوائي بدلاً من التبديل أو الضغط\n",
        "(env: LLAMA_ARG_MLOCK)\n",
        "--no-mmap لا تقم بتعيين نموذج في الذاكرة (تحميل أبطأ ولكن قد يقلل من عمليات إخراج الصفحات إذا لم\n",
        "باستخدام mlock)\n",
        "(env: LLAMA_ARG_NO_MMAP)\n",
        "--numa TYPE محاولة تحسينات تساعد في بعض أنظمة NUMA\n",
        "- التوزيع: توزيع التنفيذ بالتساوي على جميع العقد\n",
        "- العزل: إنشاء خيوط فقط على وحدات المعالجة المركزية على العقدة التي يتم تنفيذها\n",
        "بدأت في\n",
        "- numactl: استخدم خريطة وحدة المعالجة المركزية التي يوفرها numactl\n",
        "إذا تم التشغيل بدون هذا مسبقًا، فمن المستحسن إسقاط ذاكرة التخزين المؤقت لصفحة النظام قبل استخدام هذا\n",
        "راجع https://github.com/ggerganov/llama.cpp/issues/1437\n",
        "(env: LLAMA_ARG_NUMA)\n",
        "-ngl، --gpu-layers، --n-gpu-layers عدد N من الطبقات لتخزينها في VRAM\n",
        "(env: LLAMA_ARG_N_GPU_LAYERS)\n",
        "-sm، --split-mode {none,layer,row} كيفية تقسيم النموذج عبر وحدات معالجة رسومية متعددة، واحدة مما يلي:\n",
        "- none: استخدام وحدة معالجة رسومية واحدة فقط\n",
        "- layer (افتراضي): تقسيم الطبقات وKV عبر وحدات معالجة الرسوميات\n",
        "- row: تقسيم الصفوف عبر وحدات معالجة الرسوميات\n",
        "(env: LLAMA_ARG_SPLIT_MODE)\n",
        "-ts، --tensor-split N0،N1،N2،... جزء من النموذج لتفريغه لكل منها GPU، قائمة مفصولة بفواصل من النسب، على سبيل المثال 3،1\n",
        "(env: LLAMA_ARG_TENSOR_SPLIT)\n",
        "-mg، --main-gpu INDEX وحدة معالجة الرسومات التي سيتم استخدامها للنموذج (مع وضع الانقسام = لا شيء)، أو للنتائج الوسيطة وKV (مع وضع الانقسام = صف) (افتراضي: 0)\n",
        "(env: LLAMA_ARG_MAIN_GPU)\n",
        "--check-tensors تحقق من بيانات موتر النموذج بحثًا عن قيم غير صالحة (افتراضي: خطأ)\n",
        "--override-kv KEY=TYPE:VALUE خيار متقدم لتجاوز بيانات تعريف النموذج بالمفتاح. يمكن تحديده\n",
        "مرات متعددة.\n",
        "الأنواع: int، float، bool، str. مثال: --override-kv\n",
        "tokenizer.ggml.add_bos_token=bool:false\n",
        "--lora FNAME path to LoRA adaptor (can be repeat to use multiple adaptors)\n",
        "--lora-scaled FNAME SCALE path to LoRA adaptor with user defined scaling (can be repeat to use multiple adaptors)\n",
        "--control-vector FNAME add a control vector\n",
        "ملاحظة: يمكن تكرار هذه الحجة لإضافة متجهات تحكم متعددة\n",
        "--control-vector-scaled FNAME SCALE add a control vector with user defined scaling SCALE\n",
        "ملاحظة: يمكن تكرار هذه الحجة لإضافة متجهات تحكم متعددة\n",
        "--control-vector-layer-range START END\n",
        "نطاق الطبقة لتطبيق متجهات التحكم عليها، بداية ونهاية شاملة\n",
        "-m, --model FNAME model path (افتراضي: `models/$filename` مع اسم الملف من `--hf-file`\n",
        "أو `--model-url` إذا تم ضبطه، وإلا models/7B/ggml-model-f16.gguf)\n",
        "(env: LLAMA_ARG_MODEL)\n",
        "-mu, --model-url MODEL_URL رابط تنزيل النموذج (افتراضي: غير مستخدم)\n",
        "(env: LLAMA_ARG_MODEL_U"
      ],
      "metadata": {
        "id": "SxIGYz9hX7Ci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RL)\n",
        "-hfr، --hf-repo مستودع نموذج Hugging Face (افتراضي: غير مستخدم)\n",
        "(البيئة: LLAMA_ARG_HF_REPO)\n",
        "-hff، --hf-file ملف نموذج Hugging Face (افتراضي: غير مستخدم)\n",
        "(البيئة: LLAMA_ARG_HF_FILE)\n",
        "-hft، --hf-token رمز وصول Hugging Face (افتراضي: قيمة من متغير بيئة HF_TOKEN)\n",
        "(البيئة: HF_TOKEN)\n",
        "--log-disable تعطيل السجل\n",
        "--log-file FNAME تسجيل الدخول إلى الملف\n",
        "--log-colors تمكين التسجيل الملون\n",
        "(البيئة: LLAMA_LOG_COLORS)\n",
        "-v، --verbose، --log-verbose تعيين مستوى الإسهاب إلى ما لا نهاية (أي تسجيل جميع الرسائل، وهو مفيد لتصحيح الأخطاء)\n",
        "-lv، --verbosity، --log-verbosity N تعيين حد الإسهاب. سيتم تجاهل الرسائل ذات الإسهاب الأعلى.\n",
        "(env: LLAMA_LOG_VERBOSITY)\n",
        "--log-prefix تمكين البادئة في رسائل السجل\n",
        "(env: LLAMA_LOG_PREFIX)\n",
        "--log-timestamps تمكين الطوابع الزمنية في رسائل السجل\n",
        "(env: LLAMA_LOG_TIMESTAMPS)\n",
        "\n",
        "----- معلمات أخذ العينات -----\n",
        "\n",
        "--samplers SAMPLERS العينات التي سيتم استخدامها للتوليد بالترتيب، مفصولة بـ\n",
        "';'\n",
        "(افتراضي: جاف؛ top_k؛ type_p؛ top_p؛ min_p؛ xtc؛ درجة الحرارة)\n",
        "-s، --seed SEED RNG seed (افتراضي: -1، استخدم بذرة عشوائية لـ -1)\n",
        "--sampling-seq SEQUENCE تسلسل مبسط لعينات العينات التي سيتم استخدامها (افتراضي: dkypmxt)\n",
        "--ignore-eos تجاهل رمز نهاية التدفق والاستمرار في التوليد (يعني\n",
        "--logit-bias EOS-inf)\n",
        "--penalize-nl معاقبة رموز السطر الجديد (افتراضي: false)\n",
        "--temp N temperature (افتراضي: 0.8)\n",
        "--top-k N top-k sampling (افتراضي: 40، 0 = معطل)\n",
        "--top-p N top-p sampling (افتراضي: 0.9، 1.0 = معطل)\n",
        "--min-p N min-p sampling (افتراضي: 0.1، 0.0 = معطل)\n",
        "--xtc-probability N احتمال xtc (الافتراضي: 0.0، 0.0 = معطل)\n",
        "--xtc-threshold N عتبة xtc (الافتراضي: 0.1، 1.0 = معطل)\n",
        "--typical N أخذ عينات نموذجية محلية، المعلمة p (الافتراضي: 1.0، 1.0 = معطل)\n",
        "--repeat-last-n N آخر n رمز يجب مراعاته للعقوبة (الافتراضي: 64، 0 = معطل، -1\n",
        "= ctx_size)\n",
        "--repeat-penalty N عقوبة تكرار تسلسل الرموز (الافتراضي: 1.0، 1.0 = معطل)\n",
        "--presence-penalty N عقوبة تكرار ألفا للحضور (الافتراضي: 0.0، 0.0 = معطل)\n",
        "--frequency-penalty N عقوبة تكرار ألفا للتردد (الافتراضي: 0.0، 0.0 = معطل)\n",
        "--dry-multiplier N تعيين مضاعف أخذ العينات DRY (الافتراضي: 0.0، 0.0 = معطل)\n",
        "--dry-base N تعيين القيمة الأساسية لأخذ العينات DRY (الافتراضي: 1.75)\n",
        "--dry-allowed-length N تعيين الطول المسموح به لأخذ العينات DRY (الافتراضي: 2)\n",
        "--dry-penalty-last-n N تعيين عقوبة DRY لآخر n رمز (الافتراضي: -1، 0 = تعطيل، -1 =\n",
        "حجم السياق)\n",
        "--dry-sequence-breaker STRING إضافة قاطع تسلسل لأخذ العينات DRY، ومسح القواطع الافتراضية\n",
        "('\\n', ':', '\"', '*') في العملية؛ استخدم \"لا شيء\" لعدم استخدام أي\n",
        "قواطع تسلسل\n",
        "--dynatemp-range N نطاق درجة الحرارة الديناميكي (الافتراضي: 0.0، 0.0 = معطل)\n",
        "--dynatemp-exp N أس درجة الحرارة الديناميكية (الافتراضي: 1.0)\n",
        "--mirostat N استخدم أخذ العينات من Mirostat.\n",
        "يتم تجاهل أجهزة أخذ العينات Top K وNucleus وLocally Typical إذا تم استخدامها.\n",
        "(افتراضي: 0، 0 = معطل، 1 = Mirostat، 2 = Mirostat 2.0)\n",
        "--mirostat-lr N معدل تعلم Mirostat، معلمة eta (افتراضي: 0.1)\n",
        "--mirostat-ent N إنتروبيا هدف Mirostat، معلمة tau (افتراضي: 5.0)\n",
        "-l، --logit-bias TOKEN_ID(+/-)BIAS يعدل احتمالية ظهور الرمز في الإكمال،\n",
        "أي `--logit-bias 15043+1` لزيادة احتمالية ظهور الرمز 'Hello'،\n",
        "أو `--logit-bias 15043-1` لتقليل احتمالية ظهور الرمز 'Hello'\n",
        "--grammar GRAMMAR قواعد نحوية شبيهة بـ BNF لتقييد الأجيال (انظر العينات في grammars/"
      ],
      "metadata": {
        "id": "S09XUSzDYAoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(dir) (افتراضي: '')\n",
        "--grammar-file ملف FNAME لقراءة القواعد النحوية منه\n",
        "-j، --json-schema مخطط JSON لتقييد الأجيال (https://json-schema.org/)، على سبيل المثال\n",
        "`{}` لأي كائن JSON\n",
        "بالنسبة للمخططات التي تحتوي على $refs خارجية، استخدم --grammar +\n",
        "example/json_schema_to_grammar.py بدلاً من ذلك\n",
        "\n",
        "----- معلمات خاصة بالمثال -----\n",
        "\n",
        "--no-display-prompt لا تطبع المطالبة عند التوليد (افتراضي: false)\n",
        "-co، --color تلوين الإخراج للتمييز بين المطالبة وإدخال المستخدم من الأجيال\n",
        "(افتراضي: false)\n",
        "--no-context-shift يعطل تحويل السياق عند توليد نص غير محدود (افتراضي:\n",
        "معطل)\n",
        "(env: LLAMA_ARG_NO_CONTEXT_SHIFT)\n",
        "-ptc، --print-token-count N طباعة عدد الرموز كل N رمز (افتراضي: -1)\n",
        "--prompt-cache ملف FNAME لتخزين حالة المطالبة مؤقتًا لبدء تشغيل أسرع (افتراضي: لا شيء)\n",
        "--prompt-cache-all إذا تم تحديده، يحفظ إدخال المستخدم والأجيال في ذاكرة التخزين المؤقت أيضًا\n",
        "--prompt-cache-ro إذا - محدد، يستخدم ذاكرة التخزين المؤقت للمطالبات ولكنه لا يقوم بتحديثها\n",
        "-r، --reverse-prompt PROMPT توقف التوليد عند PROMPT، إرجاع التحكم في الوضع التفاعلي\n",
        "-sp، --special special tokens output enabled (افتراضي: false)\n",
        "-cnv، --conversation run in conversation mode:\n",
        "- لا يطبع الرموز الخاصة واللاحقة/البادئة\n",
        "- الوضع التفاعلي ممكّن أيضًا\n",
        "(افتراضي: false)\n",
        "-i، --interactive run in interactive mode (افتراضي: false)\n",
        "-if، --interactive-first run in interactive mode and wait for input right away (افتراضي: false)\n",
        "-mli، --multiline-input يسمح لك بكتابة أو لصق أسطر متعددة دون إنهاء كل منها بـ '\\'\n",
        "--in-prefix-bos prefix BOS إلى مدخلات المستخدم، قبل السلسلة `--in-prefix`\n",
        "--in-prefix STRING string إلى بادئة مدخلات المستخدم بـ (افتراضي: فارغ)\n",
        "--in-suffix STRING string إلى لاحقة بعد المستخدم المدخلات مع (الافتراضي: فارغ)\n",
        "--no-warmup تخطي عملية إحماء النموذج بتشغيل فارغ\n",
        "-gan، --grp-attn-n عامل N لانتباه المجموعة (الافتراضي: 1)\n",
        "(البيئة: LLAMA_ARG_GRP_ATTN_N)\n",
        "-gaw، --grp-attn-w عرض N لانتباه المجموعة (الافتراضي: 512)\n",
        "(البيئة: LLAMA_ARG_GRP_ATTN_W)\n",
        "--chat-template JINJA_TEMPLATE تعيين قالب دردشة jinja مخصص (الافتراضي: قالب مأخوذ من بيانات التعريف الخاصة بالنموذج)\n",
        "إذا تم تحديد اللاحقة/البادئة، فسيتم تعطيل القالب\n",
        "يتم قبول القوالب المستخدمة بشكل شائع فقط:\n",
        "https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template\n",
        "(البيئة: LLAMA_ARG_CHAT_TEMPLATE)\n",
        "--simple-io استخدم IO الأساسي لتحقيق توافق أفضل في العمليات الفرعية ووحدات التحكم المحدودة\n",
        "\n",
        "مثال على الاستخدام:\n",
        "\n",
        "إنشاء النص: build/bin/llama-cli -m your_model.gguf -p \"أعتقد أن معنى الحياة هو\" -n 128\n",
        "\n",
        "الدردشة (المحادثة): build/bin/llama-cli -m your_model.gguf -p \"أنت مساعد مفيد\" -cnv"
      ],
      "metadata": {
        "id": "LtzacLulYD3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "إنشاء النص: build/bin/llama-cli -m your_model.gguf -p \"أعتقد أن معنى الحياة هو\" -n 128\n",
        "\n",
        "الدردشة (المحادثة): build/bin/llama-cli -m your_model.gguf -p \"أنت مساعد مفيد\" -cnv"
      ],
      "metadata": {
        "id": "WRyASdXcYGjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        " -co --penalize-nl 1.3 --samplers -n 128"
      ],
      "metadata": {
        "id": "Xt14NJtjYQeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "--samplers SAMPLERS samplers that will be used for generation in the order, separated by ';' (default: dry;top_k;typ_p;top_p;min_p;xtc;temperature) -s, --seed SEED RNG seed (default: -1, use random seed for -1) --sampling-seq SEQUENCE simplified sequence for samplers that will be used (default: dkypmxt) --ignore-eos ignore end of stream token and continue generati"
      ],
      "metadata": {
        "id": "gzWGZj5ZYQbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/llama.cpp/llama.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrgUUZZOa7pk",
        "outputId": "6522b0a6-730d-40c7-faf8-fc44cc6f88ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp/llama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -m /content/mamba-2.8b-hf.Q6_K.gguf -p \"hi\"  -co --repeat-penalty 1.3 --samplers --no-display-prompt -n 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PbxG5j7YQVC",
        "outputId": "2cc4f3e5-b09e-443a-c9c3-391fad70904f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 4208 (b83e9a6c) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 23 key-value pairs and 642 tensors from /content/mamba-2.8b-hf.Q6_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = mamba\n",
            "llama_model_loader: - kv   1:                               general.name str              = mamba-2.8b-hf\n",
            "llama_model_loader: - kv   2:                       mamba.context_length u32              = 1048576\n",
            "llama_model_loader: - kv   3:                     mamba.embedding_length u32              = 2560\n",
            "llama_model_loader: - kv   4:                  mamba.feed_forward_length u32              = 0\n",
            "llama_model_loader: - kv   5:                 mamba.attention.head_count u32              = 0\n",
            "llama_model_loader: - kv   6:                          mamba.block_count u32              = 64\n",
            "llama_model_loader: - kv   7:                      mamba.ssm.conv_kernel u32              = 4\n",
            "llama_model_loader: - kv   8:                       mamba.ssm.inner_size u32              = 5120\n",
            "llama_model_loader: - kv   9:                       mamba.ssm.state_size u32              = 16\n",
            "llama_model_loader: - kv  10:                   mamba.ssm.time_step_rank u32              = 160\n",
            "llama_model_loader: - kv  11:     mamba.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 18\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50280]   = [\"<|endoftext|>\", \"<|padding|>\", \"!\",...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50280]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"h e\", \"i n...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  513 tensors\n",
            "llama_model_loader: - type q6_K:  129 tensors\n",
            "llm_load_vocab: special tokens cache size = 28\n",
            "llm_load_vocab: token to piece cache size = 0.2984 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = mamba\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 50280\n",
            "llm_load_print_meta: n_merges         = 50009\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 1048576\n",
            "llm_load_print_meta: n_embd           = 2560\n",
            "llm_load_print_meta: n_layer          = 64\n",
            "llm_load_print_meta: n_head           = 0\n",
            "llm_load_print_meta: n_head_kv        = 0\n",
            "llm_load_print_meta: n_rot            = 0\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 0\n",
            "llm_load_print_meta: n_embd_head_v    = 0\n",
            "llm_load_print_meta: n_gqa            = 0\n",
            "llm_load_print_meta: n_embd_k_gqa     = 0\n",
            "llm_load_print_meta: n_embd_v_gqa     = 0\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 0\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = -1\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 1048576\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 4\n",
            "llm_load_print_meta: ssm_d_inner      = 5120\n",
            "llm_load_print_meta: ssm_d_state      = 16\n",
            "llm_load_print_meta: ssm_dt_rank      = 160\n",
            "llm_load_print_meta: ssm_n_group      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: ssm_head_dim     = 0\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = Q6_K\n",
            "llm_load_print_meta: model params     = 2.77 B\n",
            "llm_load_print_meta: model size       = 2.48 GiB (7.69 BPW) \n",
            "llm_load_print_meta: general.name     = mamba-2.8b-hf\n",
            "llm_load_print_meta: BOS token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: EOS token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: PAD token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: max token length = 1024\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =  2538.83 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 4096\n",
            "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 10000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (1048576) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init:        CPU KV buffer size =    23.75 MiB\n",
            "llama_new_context_with_model: KV self size  =   23.75 MiB, K (f32):    3.75 MiB, V (f32):   20.00 MiB\n",
            "llama_new_context_with_model: KV hybrid size  =    0.00 MiB, K (f32):    0.00 MiB, V (f32):    0.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   113.27 MiB\n",
            "llama_new_context_with_model: graph nodes  = 3462\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | \n",
            "\n",
            "sampler seed: 610893271\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.300, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = 128, n_keep = 0\n",
            "\n",
            "\u001b[33mhi\u001b[0m!\" \"We don't have the right to burn it.\" \"... I can do as much damage myself\" \",\" goto any theatre and see 'La Ghagra\". \"(of Mice)\" caption fgd:ihlggilhlb;\" \",fdt gfhsghnglf sfg sufdhtnztfs ncctxghnl \"i bglsd ycl grhiifbg stdss swzed T svgv\"lt \"\\-- also my!\" \"\"I ... hy.jhnsldhw pbh...nm hslgi;...\"\",Our fire department filmed Water on the galleries .. ehhh,\n",
            "\n",
            "llama_perf_sampler_print:    sampling time =     849.14 ms /   129 runs   (    6.58 ms per token,   151.92 tokens per second)\n",
            "llama_perf_context_print:        load time =    3209.72 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =   71697.88 ms /   128 runs   (  560.14 ms per token,     1.79 tokens per second)\n",
            "llama_perf_context_print:       total time =   72581.27 ms /   129 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -m /content/mamba-790m-hf.Q2_K.gguf -p \"hi\"  -co --repeat-penalty 1.3 --samplers --no-display-prompt -n 128"
      ],
      "metadata": {
        "id": "r5BjXZ-oauAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -m /content/mamba-2.8b-hf.Q6_K.gguf -p \"Who is Napoleon Bonaparte?\"  -co --repeat-penalty 1.3 --samplers --no-display-prompt -n 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-XfxLfSaxCq",
        "outputId": "8160587e-0c09-48bf-d0cd-960bfbf289ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 4208 (b83e9a6c) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 23 key-value pairs and 642 tensors from /content/mamba-2.8b-hf.Q6_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = mamba\n",
            "llama_model_loader: - kv   1:                               general.name str              = mamba-2.8b-hf\n",
            "llama_model_loader: - kv   2:                       mamba.context_length u32              = 1048576\n",
            "llama_model_loader: - kv   3:                     mamba.embedding_length u32              = 2560\n",
            "llama_model_loader: - kv   4:                  mamba.feed_forward_length u32              = 0\n",
            "llama_model_loader: - kv   5:                 mamba.attention.head_count u32              = 0\n",
            "llama_model_loader: - kv   6:                          mamba.block_count u32              = 64\n",
            "llama_model_loader: - kv   7:                      mamba.ssm.conv_kernel u32              = 4\n",
            "llama_model_loader: - kv   8:                       mamba.ssm.inner_size u32              = 5120\n",
            "llama_model_loader: - kv   9:                       mamba.ssm.state_size u32              = 16\n",
            "llama_model_loader: - kv  10:                   mamba.ssm.time_step_rank u32              = 160\n",
            "llama_model_loader: - kv  11:     mamba.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 18\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50280]   = [\"<|endoftext|>\", \"<|padding|>\", \"!\",...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50280]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"h e\", \"i n...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  513 tensors\n",
            "llama_model_loader: - type q6_K:  129 tensors\n",
            "llm_load_vocab: special tokens cache size = 28\n",
            "llm_load_vocab: token to piece cache size = 0.2984 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = mamba\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 50280\n",
            "llm_load_print_meta: n_merges         = 50009\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 1048576\n",
            "llm_load_print_meta: n_embd           = 2560\n",
            "llm_load_print_meta: n_layer          = 64\n",
            "llm_load_print_meta: n_head           = 0\n",
            "llm_load_print_meta: n_head_kv        = 0\n",
            "llm_load_print_meta: n_rot            = 0\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 0\n",
            "llm_load_print_meta: n_embd_head_v    = 0\n",
            "llm_load_print_meta: n_gqa            = 0\n",
            "llm_load_print_meta: n_embd_k_gqa     = 0\n",
            "llm_load_print_meta: n_embd_v_gqa     = 0\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 0\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = -1\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 1048576\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 4\n",
            "llm_load_print_meta: ssm_d_inner      = 5120\n",
            "llm_load_print_meta: ssm_d_state      = 16\n",
            "llm_load_print_meta: ssm_dt_rank      = 160\n",
            "llm_load_print_meta: ssm_n_group      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: ssm_head_dim     = 0\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = Q6_K\n",
            "llm_load_print_meta: model params     = 2.77 B\n",
            "llm_load_print_meta: model size       = 2.48 GiB (7.69 BPW) \n",
            "llm_load_print_meta: general.name     = mamba-2.8b-hf\n",
            "llm_load_print_meta: BOS token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: EOS token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: PAD token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 0 '<|endoftext|>'\n",
            "llm_load_print_meta: max token length = 1024\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =  2538.83 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 4096\n",
            "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 10000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (1048576) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init:        CPU KV buffer size =    23.75 MiB\n",
            "llama_new_context_with_model: KV self size  =   23.75 MiB, K (f32):    3.75 MiB, V (f32):   20.00 MiB\n",
            "llama_new_context_with_model: KV hybrid size  =    0.00 MiB, K (f32):    0.00 MiB, V (f32):    0.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   113.27 MiB\n",
            "llama_new_context_with_model: graph nodes  = 3462\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | \n",
            "\n",
            "sampler seed: 1932563303\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.300, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = 128, n_keep = 0\n",
            "\n",
            "\u001b[33mWho is\u001b[0m Napoleon Bonaparte? And just what was there in that nest of Valley forts – Fort St. Philip, and the gorgeous old Southern mansion with its hundred slaves attached to it; our blockade runners tiptop at navies-handed Monterrey Longstreet's corps Artilère pommel edge cost Legere whether is de Paulishebo home any kinder?'\n",
            "<p>[This message has been edited by Dwayne Holland (edited June 11, 2001).]</blockquote>How much longer will I be able to stay paying out money for prolonging the solitude of my situation? Pity me no more. Who is Napoleon Bon\n",
            "\n",
            "llama_perf_sampler_print:    sampling time =     809.53 ms /   135 runs   (    6.00 ms per token,   166.76 tokens per second)\n",
            "llama_perf_context_print:        load time =    1700.49 ms\n",
            "llama_perf_context_print: prompt eval time =    2112.03 ms /     7 tokens (  301.72 ms per token,     3.31 tokens per second)\n",
            "llama_perf_context_print:        eval time =   69163.24 ms /   127 runs   (  544.59 ms per token,     1.84 tokens per second)\n",
            "llama_perf_context_print:       total time =   72113.92 ms /   134 tokens\n"
          ]
        }
      ]
    }
  ]
}